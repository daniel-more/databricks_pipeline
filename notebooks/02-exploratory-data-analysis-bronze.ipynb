{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c463218f-dae7-44c4-a1e6-4786df760dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "COMPETITION_NAME = \"store-sales-time-series-forecasting\"\n",
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "DOWNLOAD_PATH = VOLUME_TARGET_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bf8104-feaf-490d-aaea-496abd169f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python # manually putting in kaggle key\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# ---- USE YOUR REAL kaggle.json VALUES HERE ----\n",
    "kaggle_username = \"kevinalviar\"             # from kaggle.json[\"username\"]\n",
    "kaggle_token    = \"15446cb3990c4f8071488e98244d9010\"    # from kaggle.json[\"key\"]\n",
    "# -----------------------------------------------\n",
    "\n",
    "COMPETITION_NAME   = \"store-sales-time-series-forecasting\"\n",
    "VOLUME_ROOT_PATH   = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR  = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "DOWNLOAD_PATH      = VOLUME_TARGET_DIR\n",
    "ZIP_PATH           = f\"{DOWNLOAD_PATH}/{COMPETITION_NAME}.zip\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741546d6-a1fd-41de-b681-bd579238b3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90d201fc-16ed-46eb-839c-badfa0b1d58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "/Volumes/cscie103_catalog/final_project/data/raw/\n",
    "    train.csv\n",
    "    test.csv\n",
    "    stores.csv\n",
    "    oil.csv\n",
    "    holidays_events.csv\n",
    "    transactions.csv\n",
    "    sample_submission.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b50034-9c9e-413c-b41c-976e412ccde8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading in CSVs\n",
    "VOLUME_ROOT_PATH  = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "\n",
    "filenames = {\n",
    "    \"holidays_events\": \"holidays_events.csv\",\n",
    "    \"oil\": \"oil.csv\",\n",
    "    \"sample_submission\": \"sample_submission.csv\",\n",
    "    \"stores\": \"stores.csv\",\n",
    "    \"test\": \"test.csv\",\n",
    "    \"train\": \"train.csv\",\n",
    "    \"transactions\": \"transactions.csv\",\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['holidays_events']}\", header=True, inferSchema=True)\n",
    "oil_df             = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['oil']}\",              header=True, inferSchema=True)\n",
    "stores_df          = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['stores']}\",           header=True, inferSchema=True)\n",
    "transactions_df    = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['transactions']}\",     header=True, inferSchema=True)\n",
    "train_df           = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['train']}\",            header=True, inferSchema=True)\n",
    "test_df            = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['test']}\",             header=True, inferSchema=True)\n",
    "\n",
    "print(\"Loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc64103b-4d81-40e9-890b-3aaaf4f78463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are now writing Bronze Delta Tables one per CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a3b500-0762-438f-ac72-aa226d3c4501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Use your UC catalog & schema\n",
    "catalog = \"cscie103_catalog\"\n",
    "schema = \"final_project\"\n",
    "\n",
    "spark.sql(f\"USE {catalog}.{schema}\")\n",
    "\n",
    "print(f\"Writing Bronze tables into {catalog}.{schema} ...\")\n",
    "\n",
    "# ---- WRITE MANAGED DELTA TABLES (UC FRIENDLY) ----\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_train\")\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_test\")\n",
    "stores_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_stores\")\n",
    "oil_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_oil\")\n",
    "holidays_events_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_holidays_events\")\n",
    "transactions_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_transactions\")\n",
    "\n",
    "print(\"âœ… Bronze Delta tables created successfully as managed UC tables!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4938962754428469,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-exploratory-data-analysis-bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
