{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4889fd56-b245-4cc5-a180-6374f4dbfc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìì Notebook Summary\n",
    "\n",
    "This notebook configures both Databricks and local environments for data storage and processing. It sets up Kaggle credentials, downloads the store-sales-time-series-forecasting dataset, and organizes files into the specified **Databricks** volume or **local path**. Finally, it loads all CSV files into Spark DataFrames for further analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905eca62-3e76-435f-9622-0d7f61907470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa5464d-9cb5-4184-8949-6f08fd99b965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['databricks']['catalog'], config['databricks']['schema'], config[\"databricks\"][\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa559c1b-c7a2-44e0-8b2b-7a2af089deef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def running_on_databricks():\n",
    "    \"\"\"Detect if running in Databricks environment\"\"\"\n",
    "    try:\n",
    "        import pyspark.dbutils  # only available in Databricks\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_DATABRICKS = running_on_databricks()\n",
    "print(IS_DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841baafa-f0a1-4500-b016-b13859b9d60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if IS_DATABRICKS:\n",
    "    CATALOG = config[\"databricks\"][\"catalog\"]\n",
    "    SCHEMA = config[\"databricks\"][\"schema\"]\n",
    "    VOLUME = config[\"databricks\"][\"volume\"]\n",
    "\n",
    "    BASE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "else:\n",
    "    BASE_PATH = Path(config[\"local\"][\"base_path\"]) #/ config['databricks']['schema']\n",
    "\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf3f075-7191-40e2-863d-6a26429f4ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if IS_DATABRICKS:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "else:\n",
    "    BASE_PATH.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90597be-fc72-4f28-862f-e0d19632c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebde8b8-e1db-4d04-95fa-e292533a6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all imports here\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66128d0-a1c6-43d6-99d8-fa4cccb2969b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if IS_DATABRICKS:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "\n",
    "    w = WorkspaceClient()\n",
    "    SCOPE = \"kaggle\"\n",
    "\n",
    "    try:\n",
    "        w.secrets.create_scope(scope=SCOPE)\n",
    "        print(f\"‚úì Created scope '{SCOPE}'\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e).lower():\n",
    "            print(f\"Scope '{SCOPE}' already exists\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "else:\n",
    "    print(\"Running locally ‚Äî Databricks secrets not created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc0096b-9ead-4f4c-ab75-754c5d5426f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_kaggle_credentials():\n",
    "    if IS_DATABRICKS:\n",
    "        username = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-username\")\n",
    "        token = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-api-token\")\n",
    "    else:\n",
    "        # Local environment variables\n",
    "        username = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "        token = os.environ.get(\"KAGGLE_API_TOKEN\")\n",
    "        if not username or not token:\n",
    "            raise RuntimeError(\n",
    "                \"Missing Kaggle credentials locally. \"\n",
    "                \"Set KAGGLE_USERNAME and KAGGLE_API_TOKEN as environment variables \"\n",
    "                \"or use a .env file.\"\n",
    "            )\n",
    "    return username, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2907c75a-f907-4b0b-9b33-c5179cfa4ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "kaggle_username, kaggle_token = get_kaggle_credentials()\n",
    "\n",
    "# kaggle_dir = Path.home() / \".kaggle\"\n",
    "import os\n",
    "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "# kaggle_dir.mkdir(exist_ok=True)\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "# kaggle_config_path = kaggle_dir / \"kaggle.json\"\n",
    "kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
    "\n",
    "with open(kaggle_config_path, \"w\") as f:\n",
    "    json.dump({\"username\": kaggle_username, \"key\": kaggle_token}, f)\n",
    "\n",
    "# Kaggle requires permission 600\n",
    "# kaggle_config_path.chmod(0o600)\n",
    "os.chmod(kaggle_config_path, 0o600)\n",
    "\n",
    "print(f\"‚úì Created Kaggle config at: {kaggle_config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcfd53a-be37-4bfb-9b1e-f5f13aed42b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "\n",
    "# --- Paths ---\n",
    "VOLUME_TARGET_DIR = Path(BASE_PATH) # / \"data\" / \"raw\"\n",
    "VOLUME_TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List of files to check\n",
    "filenames = [\n",
    "    \"train.csv\",\n",
    "    \"test.csv\",\n",
    "    \"stores.csv\",\n",
    "    \"holidays_events.csv\",\n",
    "    \"oil.csv\",\n",
    "    \"transactions.csv\",\n",
    "    \"sample_submission.csv\"\n",
    "]\n",
    "\n",
    "# Check if all files already exist\n",
    "all_exist = all((VOLUME_TARGET_DIR / f).exists() for f in filenames)\n",
    "\n",
    "if all_exist:\n",
    "    print(\"‚ÑπÔ∏è All CSV files already exist. Skipping download.\")\n",
    "else:\n",
    "    print(\"üì• Downloading Kaggle competition files...\")\n",
    "\n",
    "    # Authenticate Kaggle API (credentials must be set up beforehand)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Download all files as a ZIP\n",
    "    competition_name = \"store-sales-time-series-forecasting\"\n",
    "    zip_path = VOLUME_TARGET_DIR / f\"{competition_name}.zip\"\n",
    "    api.competition_download_files(competition=competition_name, path=str(VOLUME_TARGET_DIR), quiet=False)\n",
    "\n",
    "    # Extract ZIP\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(VOLUME_TARGET_DIR)\n",
    "        zip_path.unlink()  # remove ZIP after extraction\n",
    "        print(f\"‚úì Downloaded and extracted files to {VOLUME_TARGET_DIR}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è ZIP file not found at {zip_path}\")\n",
    "\n",
    "# Now CSVs are guaranteed to exist; you can read them into Spark or Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4f43ec-39d2-42a7-aff0-04129b16c923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if IS_DATABRICKS:\n",
    "    # Use Databricks-provided Spark session (Spark Connect)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "else:\n",
    "    # Local Spark session\n",
    "    spark = SparkSession.builder.appName(\"local-training\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Ensure VOLUME_TARGET_DIR is a Path\n",
    "VOLUME_TARGET_DIR = Path(BASE_PATH)\n",
    "\n",
    "# Dictionary of filenames\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "# Read all CSVs into a dictionary of Spark DataFrames\n",
    "dataframes = {}\n",
    "for key, fname in filenames.items():\n",
    "    file_path = VOLUME_TARGET_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = spark.read.csv(str(file_path), header=True, inferSchema=True)\n",
    "        dataframes[key] = df\n",
    "        print(f\"‚úì Loaded '{fname}' as Spark DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "\n",
    "# Access individual DataFrames like:\n",
    "holidays_events_df = dataframes.get('holidays_events')\n",
    "oil_df = dataframes.get('oil')\n",
    "stores_df = dataframes.get('stores')\n",
    "transactions_df = dataframes.get('transactions')\n",
    "train_df = dataframes.get('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed809cd4-994e-4daa-8aaa-967d7535004f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5756684544629495,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-download-dataset-kaggle",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
