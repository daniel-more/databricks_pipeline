{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4889fd56-b245-4cc5-a180-6374f4dbfc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìì Notebook Summary\n",
    "\n",
    "This notebook configures both Databricks and local environments for data storage and processing. It sets up Kaggle credentials, downloads the store-sales-time-series-forecasting dataset, and organizes files into the specified **Databricks** volume or **local path**. Finally, it loads all CSV files into Spark DataFrames for further analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905eca62-3e76-435f-9622-0d7f61907470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa5464d-9cb5-4184-8949-6f08fd99b965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('portfolio_catalog', 'databricks_pipeline', 'data')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['databricks']['catalog'], config['databricks']['schema'], config[\"databricks\"][\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa559c1b-c7a2-44e0-8b2b-7a2af089deef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def running_on_databricks():\n",
    "    \"\"\"Detect if running in Databricks environment\"\"\"\n",
    "    try:\n",
    "        import pyspark.dbutils  # only available in Databricks\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_DATABRICKS = running_on_databricks()\n",
    "print(IS_DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2d054f-95b6-48c9-80f7-913544b9cb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfbb793-8cd8-4360-9248-a7afdbdc3399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT is_member('admins'), is_member('users'), is_member('cuenca');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841baafa-f0a1-4500-b016-b13859b9d60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IS_DATABRICKS:\n",
    "    CATALOG = config[\"databricks\"][\"catalog\"]\n",
    "    SCHEMA = config[\"databricks\"][\"schema\"]\n",
    "    VOLUME = config[\"databricks\"][\"volume\"]\n",
    "\n",
    "    BASE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "else:\n",
    "    BASE_PATH = Path(config[\"local\"][\"base_path\"]) #/ config['databricks']['schema']\n",
    "\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf3f075-7191-40e2-863d-6a26429f4ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if IS_DATABRICKS:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "else:\n",
    "    BASE_PATH.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90597be-fc72-4f28-862f-e0d19632c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (3.11)\n",
      "Requirement already satisfied: protobuf in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (6.33.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2.6.2)\n",
      "Requirement already satisfied: webencodings in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebde8b8-e1db-4d04-95fa-e292533a6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all imports here\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66128d0-a1c6-43d6-99d8-fa4cccb2969b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally ‚Äî Databricks secrets not created\n"
     ]
    }
   ],
   "source": [
    "if IS_DATABRICKS:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "\n",
    "    w = WorkspaceClient()\n",
    "    SCOPE = \"kaggle\"\n",
    "\n",
    "    try:\n",
    "        w.secrets.create_scope(scope=SCOPE)\n",
    "        print(f\"‚úì Created scope '{SCOPE}'\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e).lower():\n",
    "            print(f\"Scope '{SCOPE}' already exists\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "else:\n",
    "    print(\"Running locally ‚Äî Databricks secrets not created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc0096b-9ead-4f4c-ab75-754c5d5426f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_kaggle_credentials():\n",
    "    if IS_DATABRICKS:\n",
    "        username = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-username\")\n",
    "        token = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-api-token\")\n",
    "    else:\n",
    "        # Local environment variables\n",
    "        username = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "        token = os.environ.get(\"KAGGLE_API_TOKEN\")\n",
    "        if not username or not token:\n",
    "            raise RuntimeError(\n",
    "                \"Missing Kaggle credentials locally. \"\n",
    "                \"Set KAGGLE_USERNAME and KAGGLE_API_TOKEN as environment variables \"\n",
    "                \"or use a .env file.\"\n",
    "            )\n",
    "    return username, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2907c75a-f907-4b0b-9b33-c5179cfa4ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "kaggle_username, kaggle_token = get_kaggle_credentials()\n",
    "\n",
    "# kaggle_dir = Path.home() / \".kaggle\"\n",
    "import os\n",
    "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "# kaggle_dir.mkdir(exist_ok=True)\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "# kaggle_config_path = kaggle_dir / \"kaggle.json\"\n",
    "kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
    "\n",
    "with open(kaggle_config_path, \"w\") as f:\n",
    "    json.dump({\"username\": kaggle_username, \"key\": kaggle_token}, f)\n",
    "\n",
    "# Kaggle requires permission 600\n",
    "# kaggle_config_path.chmod(0o600)\n",
    "os.chmod(kaggle_config_path, 0o600)\n",
    "\n",
    "print(f\"‚úì Created Kaggle config at: {kaggle_config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcfd53a-be37-4bfb-9b1e-f5f13aed42b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "\n",
    "# --- Paths ---\n",
    "VOLUME_TARGET_DIR = Path(BASE_PATH) # / \"data\" / \"raw\"\n",
    "VOLUME_TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List of files to check\n",
    "filenames = [\n",
    "    \"train.csv\",\n",
    "    \"test.csv\",\n",
    "    \"stores.csv\",\n",
    "    \"holidays_events.csv\",\n",
    "    \"oil.csv\",\n",
    "    \"transactions.csv\",\n",
    "    \"sample_submission.csv\"\n",
    "]\n",
    "\n",
    "# Check if all files already exist\n",
    "all_exist = all((VOLUME_TARGET_DIR / f).exists() for f in filenames)\n",
    "\n",
    "if all_exist:\n",
    "    print(\"‚ÑπÔ∏è All CSV files already exist. Skipping download.\")\n",
    "else:\n",
    "    print(\"üì• Downloading Kaggle competition files...\")\n",
    "\n",
    "    # Authenticate Kaggle API (credentials must be set up beforehand)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Download all files as a ZIP\n",
    "    competition_name = \"store-sales-time-series-forecasting\"\n",
    "    zip_path = VOLUME_TARGET_DIR / f\"{competition_name}.zip\"\n",
    "    api.competition_download_files(competition=competition_name, path=str(VOLUME_TARGET_DIR), quiet=False)\n",
    "\n",
    "    # Extract ZIP\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(VOLUME_TARGET_DIR)\n",
    "        zip_path.unlink()  # remove ZIP after extraction\n",
    "        print(f\"‚úì Downloaded and extracted files to {VOLUME_TARGET_DIR}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è ZIP file not found at {zip_path}\")\n",
    "\n",
    "# Now CSVs are guaranteed to exist; you can read them into Spark or Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4f43ec-39d2-42a7-aff0-04129b16c923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if IS_DATABRICKS:\n",
    "    # Use Databricks-provided Spark session (Spark Connect)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "else:\n",
    "    # Local Spark session\n",
    "    spark = SparkSession.builder.appName(\"local-training\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Ensure VOLUME_TARGET_DIR is a Path\n",
    "VOLUME_TARGET_DIR = Path(BASE_PATH)\n",
    "\n",
    "# Dictionary of filenames\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "# Read all CSVs into a dictionary of Spark DataFrames\n",
    "dataframes = {}\n",
    "for key, fname in filenames.items():\n",
    "    file_path = VOLUME_TARGET_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = spark.read.csv(str(file_path), header=True, inferSchema=True)\n",
    "        dataframes[key] = df\n",
    "        print(f\"‚úì Loaded '{fname}' as Spark DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "\n",
    "# Access individual DataFrames like:\n",
    "holidays_events_df = dataframes.get('holidays_events')\n",
    "oil_df = dataframes.get('oil')\n",
    "stores_df = dataframes.get('stores')\n",
    "transactions_df = dataframes.get('transactions')\n",
    "train_df = dataframes.get('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed809cd4-994e-4daa-8aaa-967d7535004f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4924170153543541,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-download-dataset-kaggle",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
