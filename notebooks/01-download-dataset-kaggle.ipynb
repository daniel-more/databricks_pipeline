{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ef88d2-faa1-41c0-9a41-177dec2356d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook serves as a sandbox to provide exploratory data analysis in preparation for ERD & E2E workflow specifications.\n",
    "It will:\n",
    "1. Create schema & volume if needed.\n",
    "2. Fetch data from [Kaggle competition](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview).\n",
    "3. Create respective tables per csv file.\n",
    "\n",
    "... (wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa5464d-9cb5-4184-8949-6f08fd99b965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('portfolio_catalog', 'databricks_pipeline', 'data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['databricks']['catalog'], config['databricks']['schema'], config[\"databricks\"][\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa559c1b-c7a2-44e0-8b2b-7a2af089deef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def running_on_databricks():\n",
    "    try:\n",
    "        import pyspark.dbutils  # only available in Databricks\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_DATABRICKS = running_on_databricks()\n",
    "print(IS_DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841baafa-f0a1-4500-b016-b13859b9d60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if IS_DATABRICKS:\n",
    "    CATALOG = config[\"databricks\"][\"catalog\"]\n",
    "    SCHEMA = config[\"databricks\"][\"schema\"]\n",
    "    VOLUME = config[\"databricks\"][\"volume\"]\n",
    "\n",
    "    BASE_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "else:\n",
    "    BASE_PATH = Path(config[\"local\"][\"base_path\"]) #/ config['databricks']['schema']\n",
    "\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf3f075-7191-40e2-863d-6a26429f4ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if IS_DATABRICKS:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n",
    "else:\n",
    "    BASE_PATH.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90597be-fc72-4f28-862f-e0d19632c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (6.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (3.4.4)\n",
      "Requirement already satisfied: idna in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (3.11)\n",
      "Requirement already satisfied: protobuf in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (6.33.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (2.6.2)\n",
      "Requirement already satisfied: webencodings in /Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages (from kaggle) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebde8b8-e1db-4d04-95fa-e292533a6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# all imports here\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66128d0-a1c6-43d6-99d8-fa4cccb2969b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally ‚Äî Databricks secrets not created\n"
     ]
    }
   ],
   "source": [
    "if IS_DATABRICKS:\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "\n",
    "    w = WorkspaceClient()\n",
    "    SCOPE = \"kaggle\"\n",
    "\n",
    "    try:\n",
    "        w.secrets.create_scope(scope=SCOPE)\n",
    "        print(f\"‚úì Created scope '{SCOPE}'\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e).lower():\n",
    "            print(f\"Scope '{SCOPE}' already exists\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "else:\n",
    "    print(\"Running locally ‚Äî Databricks secrets not created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc0096b-9ead-4f4c-ab75-754c5d5426f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_kaggle_credentials():\n",
    "    if IS_DATABRICKS:\n",
    "        username = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-username\")\n",
    "        token = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-api-token\")\n",
    "    else:\n",
    "        # Local environment variables\n",
    "        username = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "        token = os.environ.get(\"KAGGLE_API_TOKEN\")\n",
    "        if not username or not token:\n",
    "            raise RuntimeError(\n",
    "                \"Missing Kaggle credentials locally. \"\n",
    "                \"Set KAGGLE_USERNAME and KAGGLE_API_TOKEN as environment variables \"\n",
    "                \"or use a .env file.\"\n",
    "            )\n",
    "    return username, token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2907c75a-f907-4b0b-9b33-c5179cfa4ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created Kaggle config at: /Users/daniel/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "kaggle_username, kaggle_token = get_kaggle_credentials()\n",
    "\n",
    "# kaggle_dir = Path.home() / \".kaggle\"\n",
    "import os\n",
    "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "# kaggle_dir.mkdir(exist_ok=True)\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "# kaggle_config_path = kaggle_dir / \"kaggle.json\"\n",
    "kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
    "\n",
    "with open(kaggle_config_path, \"w\") as f:\n",
    "    json.dump({\"username\": kaggle_username, \"key\": kaggle_token}, f)\n",
    "\n",
    "# Kaggle requires permission 600\n",
    "# kaggle_config_path.chmod(0o600)\n",
    "os.chmod(kaggle_config_path, 0o600)\n",
    "\n",
    "print(f\"‚úì Created Kaggle config at: {kaggle_config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcfd53a-be37-4bfb-9b1e-f5f13aed42b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è All CSV files already exist. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "\n",
    "# --- Paths ---\n",
    "VOLUME_TARGET_DIR = Path(BASE_PATH) # / \"data\" / \"raw\"\n",
    "VOLUME_TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# List of files to check\n",
    "filenames = [\n",
    "    \"train.csv\",\n",
    "    \"test.csv\",\n",
    "    \"stores.csv\",\n",
    "    \"holidays_events.csv\",\n",
    "    \"oil.csv\",\n",
    "    \"transactions.csv\",\n",
    "    \"sample_submission.csv\"\n",
    "]\n",
    "\n",
    "# Check if all files already exist\n",
    "all_exist = all((VOLUME_TARGET_DIR / f).exists() for f in filenames)\n",
    "\n",
    "if all_exist:\n",
    "    print(\"‚ÑπÔ∏è All CSV files already exist. Skipping download.\")\n",
    "else:\n",
    "    print(\"üì• Downloading Kaggle competition files...\")\n",
    "\n",
    "    # Authenticate Kaggle API (credentials must be set up beforehand)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Download all files as a ZIP\n",
    "    competition_name = \"store-sales-time-series-forecasting\"\n",
    "    zip_path = VOLUME_TARGET_DIR / f\"{competition_name}.zip\"\n",
    "    api.competition_download_files(competition=competition_name, path=str(VOLUME_TARGET_DIR), quiet=False)\n",
    "\n",
    "    # Extract ZIP\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(VOLUME_TARGET_DIR)\n",
    "        zip_path.unlink()  # remove ZIP after extraction\n",
    "        print(f\"‚úì Downloaded and extracted files to {VOLUME_TARGET_DIR}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è ZIP file not found at {zip_path}\")\n",
    "\n",
    "# Now CSVs are guaranteed to exist; you can read them into Spark or Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4f43ec-39d2-42a7-aff0-04129b16c923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 01:52:54 WARN Utils: Your hostname, daniels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.88.117 instead (on interface en0)\n",
      "25/12/22 01:52:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/22 01:52:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 'holidays_events.csv' as Spark DataFrame with 350 rows and 6 columns\n",
      "‚úì Loaded 'oil.csv' as Spark DataFrame with 1218 rows and 2 columns\n",
      "‚úì Loaded 'sample_submission.csv' as Spark DataFrame with 28512 rows and 2 columns\n",
      "‚úì Loaded 'stores.csv' as Spark DataFrame with 54 rows and 5 columns\n",
      "‚úì Loaded 'test.csv' as Spark DataFrame with 28512 rows and 5 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 'train.csv' as Spark DataFrame with 3000888 rows and 6 columns\n",
      "‚úì Loaded 'transactions.csv' as Spark DataFrame with 83488 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if IS_DATABRICKS:\n",
    "    # Use Databricks-provided Spark session (Spark Connect)\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "else:\n",
    "    # Local Spark session\n",
    "    spark = SparkSession.builder.appName(\"local-training\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Ensure VOLUME_TARGET_DIR is a Path\n",
    "VOLUME_TARGET_DIR = Path(BASE_PATH)\n",
    "\n",
    "# Dictionary of filenames\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "# Read all CSVs into a dictionary of Spark DataFrames\n",
    "dataframes = {}\n",
    "for key, fname in filenames.items():\n",
    "    file_path = VOLUME_TARGET_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = spark.read.csv(str(file_path), header=True, inferSchema=True)\n",
    "        dataframes[key] = df\n",
    "        print(f\"‚úì Loaded '{fname}' as Spark DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "\n",
    "# Access individual DataFrames like:\n",
    "holidays_events_df = dataframes.get('holidays_events')\n",
    "oil_df = dataframes.get('oil')\n",
    "stores_df = dataframes.get('stores')\n",
    "transactions_df = dataframes.get('transactions')\n",
    "train_df = dataframes.get('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed809cd4-994e-4daa-8aaa-967d7535004f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5756684544629495,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-download-dataset-kaggle",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "databricks_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
