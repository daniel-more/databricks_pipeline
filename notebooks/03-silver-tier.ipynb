{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd9df89-c192-4e57-a421-5d080e3d64a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### ðŸ““Notebook Summary\n",
    "\n",
    "This notebook implements a **robust ETL pipeline** for train and test datasets, including cleaning, deduplication, and silver-layer table creation. \n",
    "\n",
    "It enriches the data by joining **stores, holidays, and oil prices**, adds temporal and salary-day features, and encodes categorical columns using **FeatureHasher and StringIndexer**. \n",
    "\n",
    "_The_ workflow ensures **consistency, scalability, and Spark Connectâ€“friendly operations**, avoiding large model serialization issues while keeping the data ready for ML modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40d1b0b-2510-42e4-b41e-ecd3ecd9a5e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, \n",
    "    DateType, BooleanType, DoubleType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e157d8eb-58f2-443e-b771-4d90a1d104f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_config(config_path=\"../config.yaml\"):\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load config\n",
    "config = load_config(\"../config.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c463218f-dae7-44c4-a1e6-4786df760dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = config['databricks']['catalog']\n",
    "SCHEMA_NAME = config['databricks']['schema']\n",
    "\n",
    "VOLUME_ROOT_PATH = config['databricks']['volume_path'] #\"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where raw csvs land after download\n",
    "VOLUME_TARGET_DIR = config['databricks']['volume_path'] # f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "\n",
    "spark.sql(f\"USE {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "class DataframeNames:\n",
    "    HOLIDAYS = \"holidays\"\n",
    "    OIL = \"oil\"\n",
    "    STORES = \"stores\"\n",
    "    TEST = \"test\"\n",
    "    TRAIN = \"train\"\n",
    "    TRANSACTIONS = \"transactions\"\n",
    "    TRAINING = \"training\"\n",
    "    TESTING = \"testing\"\n",
    "\n",
    "    ALL = [ HOLIDAYS, OIL, STORES, TEST, TRAIN, TRANSACTIONS, TRAINING ]\n",
    "\n",
    "class DataTier:\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "    def getBronzeName(tablename):\n",
    "        return DataTier.BRONZE + \"_\" + tablename\n",
    "\n",
    "    def getSilverName(tablename):\n",
    "        return DataTier.SILVER + \"_\" + tablename\n",
    "    \n",
    "    def getGoldName(tablename):\n",
    "        return DataTier.GOLD + \"_\" + tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7856694-1c8a-4404-941e-0d911c90a906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extractTransformLoad(bronze_tablename, silver_tablename, transform, create_table):\n",
    "    \"\"\"\n",
    "    :param: bronze_tablename - bronze UC table name e.g. bronze_tablename\n",
    "    :param: silver_tablename - silver UC table name e.g. silver_tablename\n",
    "    :param: checkpoint_path - volume path to checkpoint e.g. /Volumes/...\n",
    "    :param: transform - transformation function to apply to bronze table, should accept readStream\n",
    "    :param: create_table - spark sql call to create table\n",
    "\n",
    "    :return: streaming query\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Reading from bronze table: \" + bronze_tablename)\n",
    "    read_stream_df = spark.read.format(\"delta\").table(bronze_tablename)\n",
    "    print(f\"Read {read_stream_df.count()} records from bronze table.\")\n",
    "\n",
    "    print(\"Applying transformation(s)...\")\n",
    "    transformed_df = transform(read_stream_df)\n",
    "    print(f\"After transformation, there are {transformed_df.count()} records.\")\n",
    "\n",
    "    print(f\"Creating {silver_tablename} table.\")\n",
    "    create_table()\n",
    "\n",
    "    print(\"Writing to silver table: \" + silver_tablename + \"...\")\n",
    "    transformed_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(silver_tablename)\n",
    "\n",
    "    print(f\"Silver table {silver_tablename} is written. Schema: \")\n",
    "    transformed_df.printSchema()\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32316e76-8bef-4b4c-af6a-4a09cbb4e29b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def extractTransformLoad(bronze_tablename, silver_tablename, transform, create_table):\n",
    "    \"\"\"\n",
    "    Batch ETL from bronze to silver Delta tables\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Reading from bronze table: {bronze_tablename}\")\n",
    "    df = spark.read.format(\"delta\").table(bronze_tablename)\n",
    "\n",
    "    print(\"Applying transformation(s)...\")\n",
    "    transformed_df = transform(df)\n",
    "\n",
    "    print(f\"Creating silver table if needed: {silver_tablename}\")\n",
    "    create_table()\n",
    "\n",
    "    print(f\"Writing to silver table: {silver_tablename}\")\n",
    "    (\n",
    "        transformed_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .saveAsTable(silver_tablename)\n",
    "    )\n",
    "\n",
    "    print(f\"Silver table {silver_tablename} written successfully.\")\n",
    "    transformed_df.printSchema()\n",
    "\n",
    "    return transformed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e6725f-335f-4703-8701-aeb50a8fc18b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def extractTransformLoad(bronze_tablename, silver_tablename, transform, create_table):\n",
    "    \"\"\"\n",
    "    Simple ETL: read bronze table, apply transformation, write silver table,\n",
    "    with basic metrics reporting.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Reading from bronze table: {bronze_tablename}\")\n",
    "    df = spark.read.format(\"delta\").table(bronze_tablename)\n",
    "\n",
    "    # Simple pre-transform metrics\n",
    "    print(f\"Pre-transform row count: {df.count()}\")\n",
    "    if \"sales\" in df.columns:\n",
    "        total_sales = df.agg(F.sum(\"sales\")).collect()[0][0]\n",
    "        print(f\"Pre-transform total sales: {total_sales}\")\n",
    "\n",
    "    print(\"Applying transformation(s)...\")\n",
    "    df_transformed = transform(df)\n",
    "\n",
    "    # Simple post-transform metrics\n",
    "    print(f\"Post-transform row count: {df_transformed.count()}\")\n",
    "    if \"sales\" in df_transformed.columns:\n",
    "        total_sales_after = df_transformed.agg(F.sum(\"sales\")).collect()[0][0]\n",
    "        print(f\"Post-transform total sales: {total_sales_after}\")\n",
    "\n",
    "    print(f\"Creating {silver_tablename} table.\")\n",
    "    create_table()\n",
    "\n",
    "    print(f\"Writing to silver table: {silver_tablename}...\")\n",
    "    df_transformed.write.mode(\"overwrite\").format(\"delta\").saveAsTable(silver_tablename)\n",
    "\n",
    "    print(f\"Silver table {silver_tablename} is written. Schema:\")\n",
    "    df_transformed.printSchema()\n",
    "\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a442dd-7862-45be-9a09-3b8a28b00d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building Out Silver Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b4d788-94ce-46a2-83b8-b53e7727644d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9796d5c-3732-4935-98fa-37a5d8daa236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Stores Dataframe\n",
    "bronze_tablename_stores = DataTier.getBronzeName(DataframeNames.STORES)\n",
    "silver_tablename_stores = DataTier.getSilverName(DataframeNames.STORES)\n",
    "\n",
    "\n",
    "# Transformation logic for stores\n",
    "def transform(df):\n",
    "    return (\n",
    "        df.select(\"store_nbr\", \"city\", \"state\", \"type\", \"cluster\")\n",
    "          .filter(F.col(\"store_nbr\").isNotNull())\n",
    "          .dropDuplicates([\"store_nbr\"])\n",
    "    )\n",
    "\n",
    "# Create table safely (idempotent)\n",
    "def create_table():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_tablename_stores} (\n",
    "            store_nbr INTEGER NOT NULL,\n",
    "            city STRING,\n",
    "            state STRING,\n",
    "            type STRING,\n",
    "            cluster INTEGER,\n",
    "            CONSTRAINT pk_silver_store_nbr PRIMARY KEY (store_nbr)\n",
    "        )\n",
    "        USING DELTA\n",
    "        COMMENT 'Cleaned store dimension table'\n",
    "    \"\"\")\n",
    "\n",
    "# ETL execution\n",
    "stores_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_stores,\n",
    "    silver_tablename_stores,\n",
    "    transform,\n",
    "    create_table\n",
    ")\n",
    "\n",
    "print(\"Stores silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c15497a-f376-4bd9-9cd3-53fd00e9f0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1e0229-a656-4fac-8561-5332555f404f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Holidays Dataframe\n",
    "bronze_tablename_holidays = DataTier.getBronzeName(DataframeNames.HOLIDAYS)\n",
    "silver_tablename_holidays = DataTier.getSilverName(DataframeNames.HOLIDAYS)\n",
    "\n",
    "# Preload reference data OUTSIDE the transform\n",
    "\n",
    "# All available states\n",
    "bronze_stores = spark.read.table(\n",
    "    DataTier.getBronzeName(DataframeNames.STORES)\n",
    ").select(\"state\").distinct()\n",
    "\n",
    "# Get the continous range of dates \n",
    "train_dates_df = spark.read.table(\n",
    "    DataTier.getBronzeName(DataframeNames.HOLIDAYS)\n",
    ").select(\"date\")\n",
    "\n",
    "date_bounds = train_dates_df.agg(\n",
    "    F.min(\"date\").alias(\"min_date\"),\n",
    "    F.max(\"date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "min_date = date_bounds[\"min_date\"]\n",
    "max_date = date_bounds[\"max_date\"]\n",
    "\n",
    "print(f\"Date bounds: {min_date} - {max_date}\")\n",
    "\n",
    "date_spine = (\n",
    "    spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        F.explode(\n",
    "            F.sequence(F.lit(min_date), F.lit(max_date))\n",
    "        ).alias(\"date\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d45837e-3d90-4be8-a749-6d34342e5162",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766463323333}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").table(bronze_tablename_holidays)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067460e5-dbd5-43f8-b23e-46fc3aac35c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform(holidays_events_df):\n",
    "    \"\"\"\n",
    "    Holiday calendar preparation steps:\n",
    "\n",
    "    1. Remove transferred holidays\n",
    "       - Drop rows where the holiday was transferred to another date\n",
    "       - Identified by column `transferred = 'true'`\n",
    "\n",
    "    2. Expand holidays to state level\n",
    "       a. Nationwide holidays\n",
    "          - Rows where `locale_name = 'Ecuador'`\n",
    "          - Expanded to one row per state using cross join with `bronze_stores`\n",
    "       b. State-specific holidays\n",
    "          - Rows where `locale_name != 'Ecuador'`\n",
    "          - `locale_name` is treated as the state\n",
    "\n",
    "    3. Construct holiday flag\n",
    "       - Output schema after expansion:\n",
    "         (date, state, is_holiday)\n",
    "       - `is_holiday` is set to 1 for all holidays\n",
    "\n",
    "    4. Deduplicate holiday records\n",
    "       - Ensure a single row per (date, state)\n",
    "       - Handles overlaps between nationwide and regional holidays\n",
    "\n",
    "    5. Build full calendar spine\n",
    "       - Cross join all continuous dates (`bronze_train_dates`)\n",
    "         with all states (`bronze_stores`)\n",
    "       - Guarantees complete (date, state) coverage\n",
    "\n",
    "    6. Mark non-holidays\n",
    "       - Left join expanded holidays onto the full calendar\n",
    "       - Missing matches are filled with `is_holiday = 0`\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 1. Remove transferred holidays\n",
    "    holidays_df = holidays_events_df.filter(F.col(\"transferred\") != \"true\")\n",
    "\n",
    "    # 2a. Nationwide holidays -> explode to all states\n",
    "    nationwide = (\n",
    "        holidays_df\n",
    "        .filter(F.col(\"locale_name\") == \"Ecuador\")\n",
    "        .crossJoin(bronze_stores)\n",
    "        .select(\n",
    "            \"date\",\n",
    "            F.col(\"state\"),\n",
    "            F.lit(1).alias(\"is_holiday\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2b. State-specific holidays\n",
    "    regional = (\n",
    "        holidays_df\n",
    "        .filter(F.col(\"locale_name\") != \"Ecuador\")\n",
    "        .select(\n",
    "            \"date\",\n",
    "            F.col(\"locale_name\").alias(\"state\"),\n",
    "            F.lit(1).alias(\"is_holiday\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 3 & 4. Combine and deduplicate holidays\n",
    "    holidays_expanded = (\n",
    "        nationwide\n",
    "        .unionByName(regional)\n",
    "        .dropDuplicates([\"date\", \"state\"])\n",
    "    )\n",
    "\n",
    "    # 5 & 6. Build full calendar and mark non-holidays\n",
    "    holidays_calendar = (\n",
    "        date_spine\n",
    "        .crossJoin(bronze_stores)\n",
    "        .join(\n",
    "            holidays_expanded,\n",
    "            on=[\"date\", \"state\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .fillna(0, subset=[\"is_holiday\"])\n",
    "    )\n",
    "\n",
    "    return holidays_calendar\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {silver_tablename_holidays}\")\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {silver_tablename_holidays} (\n",
    "            date DATE NOT NULL,\n",
    "            state STRING NOT NULL,\n",
    "            is_holiday INTEGER NOT NULL,\n",
    "            \n",
    "            CONSTRAINT pk_silver_holiday \n",
    "            PRIMARY KEY (date, state)\n",
    "        )\n",
    "        USING DELTA\n",
    "        COMMENT 'Processed holiday calendar exploded by state with binary holiday flag. Includes all training dates with is_holiday=0 for non-holidays.';\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "extractTransformLoad(\n",
    "    bronze_tablename_holidays,\n",
    "    silver_tablename_holidays,\n",
    "    transform,\n",
    "    create_table\n",
    ")\n",
    "print(\"Holidays silver table written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "548b857a-9a10-4a1a-af2b-29b2a72ae5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To visualize the holidays\n",
    "pdf = spark.sql(\"\"\"\n",
    "    SELECT date, state, is_holiday\n",
    "    FROM silver_holidays\n",
    "    ORDER BY date, state\n",
    "\"\"\").toPandas()\n",
    "\n",
    "pivot_df = pdf.pivot(\n",
    "    index=\"date\",\n",
    "    columns=\"state\",\n",
    "    values=\"is_holiday\"\n",
    ")\n",
    "pivot_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef56f667-3230-4261-857a-eed9c6b856a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f477a4-29f2-4685-a24b-1db905a19bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oil Dataframe\n",
    "bronze_tablename_oil = DataTier.getBronzeName(DataframeNames.OIL)\n",
    "silver_tablename_oil = DataTier.getSilverName(DataframeNames.OIL)\n",
    "\n",
    "# Moves I/O outside transform\n",
    "oil_raw = spark.read.table(bronze_tablename_oil).select(\"date\")\n",
    "\n",
    "bounds = oil_raw.agg(\n",
    "    F.min(\"date\").alias(\"min_date\"),\n",
    "    F.max(\"date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "min_date = bounds[\"min_date\"]\n",
    "max_date = bounds[\"max_date\"]\n",
    "print(f\"Oil data covers dates {min_date} to {max_date}\")\n",
    "\n",
    "# This guarantees one row per calendar day.\n",
    "oil_date_spine = (\n",
    "    spark\n",
    "    .range(1)\n",
    "    .select(\n",
    "        F.explode(\n",
    "            F.sequence(F.lit(min_date), F.lit(max_date))\n",
    "        ).alias(\"date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def transform(oil_df):\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "    oil_df = (\n",
    "        oil_df\n",
    "        .select(\"date\", \"dcoilwtico\")\n",
    "        .withColumn(\"date\", F.to_date(\"date\"))\n",
    "        .withColumn(\"dcoilwtico\", F.col(\"dcoilwtico\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "    # Join to continuous date spine\n",
    "    oil_df = oil_date_spine.join(oil_df, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Forward fill\n",
    "    window_ffill = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    oil_df = oil_df.withColumn(\n",
    "        \"dcoilwtico\",\n",
    "        F.last(\"dcoilwtico\", ignorenulls=True).over(window_ffill)\n",
    "    )\n",
    "\n",
    "    # Backward fill for initial nulls\n",
    "    window_bfill = Window.orderBy(\"date\").rowsBetween(0, Window.unboundedFollowing)\n",
    "    oil_df = oil_df.withColumn(\n",
    "        \"dcoilwtico\",\n",
    "        F.first(\"dcoilwtico\", ignorenulls=True).over(window_bfill)\n",
    "    )\n",
    "\n",
    "    # Final safety: enforce NOT NULL contract\n",
    "    oil_df = oil_df.filter(F.col(\"dcoilwtico\").isNotNull())\n",
    "\n",
    "    return oil_df\n",
    "\n",
    "def create_table():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_tablename_oil} (\n",
    "            date DATE NOT NULL,\n",
    "            dcoilwtico DOUBLE NOT NULL,\n",
    "            CONSTRAINT pk_silver_oil PRIMARY KEY (date)\n",
    "        )\n",
    "        USING DELTA\n",
    "        COMMENT 'Daily WTI Crude Oil prices with continuous dates and forward-filled values'\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "extractTransformLoad(\n",
    "    bronze_tablename_oil,\n",
    "    silver_tablename_oil,\n",
    "    transform,\n",
    "    create_table\n",
    ")\n",
    "print(\"Oil silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525f8649-6b35-4ea5-a7f6-883194d5b816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c40753-eb60-4b7a-ac0c-58bddae3694d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Dataframe\n",
    "bronze_tablename_train = DataTier.getBronzeName(DataframeNames.TRAIN)\n",
    "silver_tablename_train = DataTier.getSilverName(DataframeNames.TRAIN)\n",
    "# transform = lambda df: df.dropna()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform(df):\n",
    "    \"\"\"\n",
    "    Train data cleaning:\n",
    "\n",
    "    1. Keep only rows where mandatory columns are not null\n",
    "    2. Cast to correct types\n",
    "    3. Optional: deduplicate by primary key (id)\n",
    "    \"\"\"\n",
    "    df_clean = (\n",
    "        df.filter(\n",
    "            F.col(\"id\").isNotNull() &\n",
    "            F.col(\"date\").isNotNull() &\n",
    "            F.col(\"store_nbr\").isNotNull() &\n",
    "            F.col(\"family\").isNotNull() &\n",
    "            F.col(\"sales\").isNotNull() &\n",
    "            F.col(\"onpromotion\").isNotNull()\n",
    "        )\n",
    "        .withColumn(\"sales\", F.col(\"sales\").cast(\"double\"))\n",
    "        .withColumn(\"onpromotion\", F.col(\"onpromotion\").cast(\"integer\"))\n",
    "        .withColumn(\"date\", F.to_date(\"date\"))\n",
    "        .dropDuplicates([\"id\"])\n",
    "    )\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_tablename_train} (\n",
    "            id INTEGER NOT NULL,\n",
    "            date DATE NOT NULL,\n",
    "            store_nbr INTEGER NOT NULL,\n",
    "            family STRING NOT NULL,\n",
    "            sales DOUBLE NOT NULL,\n",
    "            onpromotion INTEGER NOT NULL,\n",
    "\n",
    "            year INTEGER GENERATED ALWAYS AS (YEAR(date)),\n",
    "            month INTEGER GENERATED ALWAYS AS (MONTH(date)),\n",
    "\n",
    "            CONSTRAINT pk_train PRIMARY KEY (id),\n",
    "            CONSTRAINT fk_train_store FOREIGN KEY (store_nbr) REFERENCES {silver_tablename_stores} (store_nbr)\n",
    "        )\n",
    "        USING DELTA\n",
    "        PARTITIONED BY (year, month)\n",
    "        COMMENT 'Silver layer train data - historical sales transactions with store and product family information. Partitioned by year/month for efficient time-based queries.'\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "train_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_train,\n",
    "    silver_tablename_train,\n",
    "    transform,\n",
    "    create_table\n",
    ")\n",
    "\n",
    "print(\"Train silver table written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ede642-d977-43b2-aada-767f125e9855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test Dataframe\n",
    "bronze_tablename_test = DataTier.getBronzeName(DataframeNames.TEST)\n",
    "silver_tablename_test = DataTier.getSilverName(DataframeNames.TEST)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform_test(df):\n",
    "    \"\"\"\n",
    "    Test data cleaning:\n",
    "\n",
    "    1. Keep only rows where mandatory columns are not null\n",
    "    2. Cast to correct types\n",
    "    3. Drop duplicates if needed\n",
    "    \"\"\"\n",
    "    df_clean = (\n",
    "        df.filter(\n",
    "            F.col(\"id\").isNotNull() &\n",
    "            F.col(\"date\").isNotNull() &\n",
    "            F.col(\"store_nbr\").isNotNull() &\n",
    "            F.col(\"family\").isNotNull() &\n",
    "            F.col(\"onpromotion\").isNotNull()\n",
    "        )\n",
    "        .withColumn(\"onpromotion\", F.col(\"onpromotion\").cast(\"integer\"))\n",
    "        .withColumn(\"date\", F.to_date(\"date\"))\n",
    "        .dropDuplicates([\"id\"])\n",
    "    )\n",
    "    return df_clean\n",
    "\n",
    "def create_test_table():\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_tablename_test} (\n",
    "            id INTEGER NOT NULL,\n",
    "            date DATE NOT NULL,\n",
    "            store_nbr INTEGER NOT NULL,\n",
    "            family STRING NOT NULL,\n",
    "            onpromotion INTEGER NOT NULL,\n",
    "\n",
    "            year INTEGER GENERATED ALWAYS AS (YEAR(date)),\n",
    "            month INTEGER GENERATED ALWAYS AS (MONTH(date)),\n",
    "\n",
    "            CONSTRAINT pk_test PRIMARY KEY (id),\n",
    "            CONSTRAINT fk_test_store FOREIGN KEY (store_nbr) REFERENCES {silver_tablename_stores} (store_nbr)\n",
    "        )\n",
    "        USING DELTA\n",
    "        PARTITIONED BY (year, month)\n",
    "        COMMENT 'Silver layer test data - store and product family information, partitioned by year/month.'\n",
    "    \"\"\")\n",
    "\n",
    "# Run ETL\n",
    "test_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_test,\n",
    "    silver_tablename_test,\n",
    "    transform_test,\n",
    "    create_test_table\n",
    ")\n",
    "\n",
    "print(\"Test silver table written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7736ba28-98b1-43a3-972e-95c072863383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The function enrichTrainTest\n",
    "Allow to add store, oil and holidays to both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e670cbda-4a71-42b7-9c26-691afbd53a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import FeatureHasher, StringIndexer\n",
    "\n",
    "def enrichTrainTest(df, indexer_models=None, include_features=True):\n",
    "    if indexer_models is None:\n",
    "        indexer_models = {}\n",
    "\n",
    "    # Merge with stores\n",
    "    stores_df = spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.STORES))\n",
    "    df = df.join(stores_df, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "    # Merge with holidays\n",
    "    holidays_df = spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.HOLIDAYS))\n",
    "    df = df.join(holidays_df, on=[\"date\", \"state\"], how=\"left\").fillna({\"is_holiday\": 0})\n",
    "\n",
    "    # Merge with oil\n",
    "    oil_df = spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.OIL))\n",
    "    df = df.join(oil_df, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Drop remaining nulls\n",
    "    df = df.dropna()\n",
    "\n",
    "    if include_features:\n",
    "        categorical_columns = [\"family\", \"city\", \"state\", \"type\"]\n",
    "\n",
    "        # FeatureHasher\n",
    "        hasher = FeatureHasher(inputCols=categorical_columns, outputCol=\"hash_features\", numFeatures=1024)\n",
    "        df = hasher.transform(df)\n",
    "\n",
    "        # 6. Encode categorical features using StringIndexer\n",
    "        def stringEncoding(df, colname):\n",
    "            all_unq_vals = df.select(colname).distinct().collect()\n",
    "            print(f\"Found {len(all_unq_vals)} unique values for {colname}:\")\n",
    "            \n",
    "            encoding_dict = {} # {value: index}\n",
    "            for i in range(len(all_unq_vals)):\n",
    "                encoding_dict[all_unq_vals[i][0]] = i\n",
    "\n",
    "            encoding_df = spark.createDataFrame(\n",
    "            [(key, value) for key, value in encoding_dict.items()],\n",
    "            schema=[colname, 'strIndxer_' + colname]\n",
    "            )\n",
    "            encoding_df_name = 'strIndxer_' + colname\n",
    "            print(f\"Encoding table {encoding_df_name} created.\")\n",
    "\n",
    "            encoding_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(encoding_df_name)\n",
    "            print(f\"Encoding table {encoding_df_name} saved.\")\n",
    "\n",
    "            df = df.join(\n",
    "            encoding_df,\n",
    "            on=colname\n",
    "            )\n",
    "            print(f\"Encoding joined with source table on {colname}, new column {encoding_df_name} added.\")\n",
    "            return df\n",
    "            \n",
    "        for colname in categorical_columns:\n",
    "            df = stringEncoding(df, colname)\n",
    "        # df = stringEncoding(df, categorical_columns) \n",
    "\n",
    "        # # StringIndexer with reuse for train/test\n",
    "        # for col in categorical_columns:\n",
    "        #     output_col = f\"{col}_idx\"\n",
    "        #     if col in indexer_models:\n",
    "        #         model = indexer_models[col]\n",
    "        #     else:\n",
    "        #         indexer = StringIndexer(inputCol=col, outputCol=output_col, handleInvalid=\"keep\")\n",
    "        #         model = indexer.fit(df)\n",
    "        #         indexer_models[col] = model\n",
    "        #     df = model.transform(df)\n",
    "\n",
    "        # Temporal features\n",
    "        df = df.withColumn(\"day_of_week\", F.dayofweek(\"date\")) \\\n",
    "               .withColumn(\"day_of_month\", F.dayofmonth(\"date\")) \\\n",
    "               .withColumn(\"month\", F.month(\"date\")) \\\n",
    "               .withColumn(\"year\", F.year(\"date\"))\n",
    "\n",
    "        # Salary day\n",
    "        df = df.withColumn(\"is_salary_day\",\n",
    "                           F.when((F.dayofmonth(\"date\") == 15) | (F.dayofmonth(\"date\") == 30), 1).otherwise(0))\n",
    "\n",
    "    return df, indexer_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda28377-5894-4527-9435-8f02c8fc7062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS portfolio_catalog.databricks_pipeline.silver_training;\n",
    "DROP TABLE IF EXISTS portfolio_catalog.databricks_pipeline.silver_testing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2783732e-4065-45f6-9e86-ae9e1e9088e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.format(\"delta\") \\\n",
    "    .table(DataTier.getSilverName(DataframeNames.TRAIN)) \\\n",
    "    .select(\"date\", \"store_nbr\", \"family\", \"sales\", \"onpromotion\")\n",
    "\n",
    "train_df, indexer_models = enrichTrainTest(train_df)\n",
    "train_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\") \\\n",
    "    .format(\"delta\").saveAsTable(DataTier.getSilverName(DataframeNames.TRAINING))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c5e49e-f1dc-4004-a491-77cdab499b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = spark.read.format(\"delta\") \\\n",
    "    .table(DataTier.getSilverName(DataframeNames.TEST)) \\\n",
    "    .select(\"date\", \"store_nbr\", \"family\", \"onpromotion\")  # no sales\n",
    "\n",
    "test_df, _ = enrichTrainTest(test_df, indexer_models=indexer_models)\n",
    "test_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\") \\\n",
    "    .format(\"delta\").saveAsTable(DataTier.getSilverName(DataframeNames.TESTING))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df493114-1746-4a11-a951-8ba4c35c828b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766469844927}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * from silver_training LIMIT 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d842ee4-0e2e-4c72-b307-603bcbe38aa2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766469908847}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * from silver_testing LIMIT 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04935a13-c30f-4a83-b038-d4466937064d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DataTier.getSilverName(DataframeNames.TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e87043b-88ae-4ab5-8c93-5a2064a34fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_TARGET_DIR + '/silver_parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1a2fa0-7170-4a21-bd7d-d260050d1362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.table(\"silver_training\")\n",
    "silver_df.write.format(\"parquet\").mode(\"overwrite\").save(VOLUME_TARGET_DIR + \"/silver_parquet\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319e619c-fd71-4bf1-91fa-259e8b63f252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = spark.table(\"silver_testing\")\n",
    "silver_df.write.format(\"parquet\").mode(\"overwrite\").save(VOLUME_TARGET_DIR + \"/silver_test_parquet\")\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0e7766-2411-4f82-a5f3-3d86b4a20b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6319738474399638,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-silver-tier",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
