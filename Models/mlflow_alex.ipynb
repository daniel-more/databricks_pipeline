{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3afcdb3-1b5a-4e44-9b34-64c59ae56045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901a03c7-d14c-432d-8fa1-6918330f1918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from scipy.sparse import issparse # To check if it's a sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8941e8e2-1a5e-4020-8594-a25ba69eef99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "\n",
    "# write to Silver tier as Delta table\n",
    "silver_path = f'{VOLUME_SILVER_DIR}/training'\n",
    "training_df = spark.read.format(\"delta\").load(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656a06e4-0c4a-467c-9fa1-1a6fb3b7f09b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b796ce64-8abc-47d0-a96d-7b7d97efc122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_ffnn_model(input_dim):\n",
    "    \"\"\"Defines a simple Feedforward Neural Network.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input Layer matches the dimension of the hashed feature vector\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        # Output layer for binary classification\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3cfd78e-4790-41d7-b5f8-550fc5dbb5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. List all columns that will serve as input features\n",
    "feature_cols = [\n",
    "    \"hash_storenbr_family\",\n",
    "    \"hash_city_state_type_cluster\",\n",
    "    \"hash_state_isHoliday\",\n",
    "    \"hash_storeNbr\",\n",
    "    # Numerical features that were not hashed\n",
    "    \"onpromotion\",\n",
    "    \"transactions\"\n",
    "]\n",
    "\n",
    "# 2. Initialize the VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\" # The final single vector column\n",
    ")\n",
    "\n",
    "# 3. Apply the transformation to your DataFrame\n",
    "# Assume your existing DataFrame is named 'input_df'\n",
    "assembled_df = assembler.transform(training_df).select(\"date\", col(\"sales\").alias(\"label\"), \"features\")\n",
    "\n",
    "# Show the new 'features' column\n",
    "assembled_df.select(\"features\", \"label\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e0b784-dbcb-462d-adeb-a5aab788f675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, expr, date_add, to_date\n",
    "\n",
    "# Define the chronological split date\n",
    "CUTOFF_DATE = \"2016-01-01\"\n",
    "TRAIN_PATH = f\"{VOLUME_SILVER_DIR}/ffnn_sales_training_data\"\n",
    "TEST_PATH = f\"{VOLUME_SILVER_DIR}/ffnn_sales_test_data\"\n",
    "\n",
    "# Split Spark DataFrames\n",
    "train_spark_df = assembled_df.filter(col(\"date\") < lit(CUTOFF_DATE))\n",
    "test_spark_df = assembled_df.filter(col(\"date\") >= lit(CUTOFF_DATE))\n",
    "\n",
    "print(f\"Training records: {train_spark_df.count()}\")\n",
    "print(f\"Test records: {test_spark_df.count()}\")\n",
    "\n",
    "# Save for Petastorm consumption (crucial step to avoid NumPy)\n",
    "# dbutils.fs.rm(TRAIN_PATH, recurse=True)\n",
    "# dbutils.fs.rm(TEST_PATH, recurse=True)\n",
    "train_spark_df.write.format(\"delta\").save(TRAIN_PATH)\n",
    "test_spark_df.write.format(\"delta\").save(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fe00d68-7f1f-4502-ac4f-9811ae15c7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import horovod.tensorflow.keras as hvd\n",
    "from petastorm.spark import SparkDatasetConverter\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# --- Horovod Training Function (This runs on every worker) ---\n",
    "def train_ffnn_horovod(input_dim, train_path, test_path, epochs, batch_size):\n",
    "    # Initialize Horovod\n",
    "    hvd.init()\n",
    "    \n",
    "    # Set GPU affinity if applicable\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "\n",
    "    # --- 1. Model Definition (Regression) ---\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='linear') # Regression Output\n",
    "    ])\n",
    "\n",
    "    # --- 2. Distributed Optimizer and Compilation ---\n",
    "    # Scale learning rate by the number of workers\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001 * hvd.size())\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # --- 3. Petastorm Data Loading (Distributed) ---\n",
    "    converter = SparkDatasetConverter(spark)\n",
    "    \n",
    "    with converter.make_tf_dataset(spark.read.parquet(train_path)) as train_dataset, \\\n",
    "         converter.make_tf_dataset(spark.read.parquet(test_path)) as val_dataset:\n",
    "        \n",
    "        # Define how Petastorm maps columns to model input\n",
    "        feature_columns = ['features']\n",
    "        target_column = 'label'\n",
    "\n",
    "        def map_fn(x):\n",
    "            return x[feature_columns[0]], x[target_column]\n",
    "\n",
    "        # Configure Petastorm to work with Horovod\n",
    "        train_hvd_dataset = train_dataset.map(map_fn).shard(hvd.size(), hvd.rank()).shuffle(100).batch(batch_size)\n",
    "        val_hvd_dataset = val_dataset.map(map_fn).shard(hvd.size(), hvd.rank()).batch(batch_size)\n",
    "\n",
    "        # --- 4. Training ---\n",
    "        callbacks = [\n",
    "            hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "            hvd.callbacks.MetricAverageCallback(),\n",
    "        ]\n",
    "        \n",
    "        # Horovod training requires steps per epoch, not just epochs\n",
    "        train_steps = int(train_spark_df.count() / hvd.size() / batch_size)\n",
    "        val_steps = int(test_spark_df.count() / hvd.size() / batch_size)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_hvd_dataset,\n",
    "            steps_per_epoch=train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_hvd_dataset,\n",
    "            validation_steps=val_steps,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1 if hvd.rank() == 0 else 0 # Only rank 0 prints output\n",
    "        )\n",
    "\n",
    "    # --- 5. MLflow Logging and Saving (Only on rank 0) ---\n",
    "    if hvd.rank() == 0:\n",
    "        with mlflow.start_run() as run:\n",
    "            # Log metrics\n",
    "            mlflow.log_param(\"input_dim\", input_dim)\n",
    "            mlflow.log_metric(\"final_val_mae\", history.history['val_mae'][-1])\n",
    "            \n",
    "            # Save the trained model\n",
    "            mlflow.tensorflow.log_model(model, \"model\", registered_model_name=\"Sales_FFNN_Model\")\n",
    "            print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "\n",
    "# --- 6. Launch the Distributed Training ---\n",
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    "\n",
    "# NOTE: Set num_slots to the number of workers you want to use (e.g., number of cores)\n",
    "# For local testing, you might set this to the number of local cores.\n",
    "runner = MirroredStrategyRunner(num_slots=4) \n",
    "\n",
    "runner.run(\n",
    "    train_ffnn_horovod, \n",
    "    input_dim=INPUT_DIMENSION, \n",
    "    train_path=TRAIN_PATH, \n",
    "    test_path=TEST_PATH, \n",
    "    epochs=10, \n",
    "    batch_size=64\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow_alex",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
