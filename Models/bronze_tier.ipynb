{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0011eb24-ebb5-4f88-b31a-1f5327cc87d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cafae124-28b7-45a6-8f49-8f1c61029985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where raw csvs land after download\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "# raw data\n",
    "VOLUME_BRONZE_DIR = f\"{VOLUME_ROOT_PATH}/bronze\"\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "# place where final data is written\n",
    "VOLUME_GOLD_DIR = f\"{VOLUME_ROOT_PATH}/gold\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_TARGET_DIR, VOLUME_BRONZE_DIR, VOLUME_SILVER_DIR, VOLUME_GOLD_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdff67d-c731-473f-9416-9305b24b004c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from local volumes\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "# oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n",
    "\n",
    "test_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('test')}\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83fcd92-c459-4e20-8929-941691421109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write all dfs as they are into bronze\n",
    "for df, name in zip([holidays_events_df, stores_df, transactions_df, train_df, test_df ], ['holidays', 'stores', 'transactions', 'train', 'test']):\n",
    "  # delete any filename with the same name beforehand\n",
    "  dbutils.fs.rm(f\"{VOLUME_BRONZE_DIR}/{name}\", True)\n",
    "  df.write.mode(\"overwrite\").parquet(f\"{VOLUME_BRONZE_DIR}/{name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_tier",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
