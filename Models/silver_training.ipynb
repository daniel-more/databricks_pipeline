{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790a8917-6ba0-4841-a6aa-4e084b6dd797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.ml.feature import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ff2357-8377-4b07-a5e1-66198f1bf18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_SILVER_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb7a4c5-6c0c-4d54-823a-c93650eb99e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_filenames = {\n",
    "    'holidays': 'holidays',\n",
    "    'stores': 'stores',\n",
    "    'train': 'train',\n",
    "    'transactions': 'transactions',\n",
    "    'test': 'test'\n",
    "}\n",
    "\n",
    "# read from Bronze tier as Delta tables\n",
    "holidays_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('holidays')}\")\n",
    "stores_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('stores')}\")\n",
    "train_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('train')}\")\n",
    "transactions_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('transactions')}\")\n",
    "# test_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('test')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d4870e-67c4-4f7e-8ea7-aa780fc0a8da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def smart_na_drop(df):\n",
    "    \"\"\"\n",
    "    Drops all rows with any null values in columns.\n",
    "    \"\"\"\n",
    "    before = df.count()\n",
    "    df = df.dropna()\n",
    "    after = df.count()\n",
    "    print(f\"dropped {before - after} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c70fb24-52ae-45d0-bab9-85ac3023fa55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df.printSchema()\n",
    "stores_df.printSchema()\n",
    "train_df.printSchema()\n",
    "transactions_df.printSchema()\n",
    "# test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbd101c-f3a3-47cb-9dd4-12a5af3874f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge train & transactions by date, store_nbr\n",
    "training_df = train_df.join(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# for some families of products sales were none on the given day, transactions df contains nulls, filling them with 0\n",
    "training_df = training_df.withColumn(\n",
    "    'transactions',\n",
    "    F.when(F.col('sales') == 0, 0).otherwise(F.col('transactions'))\n",
    ")\n",
    "training_df = smart_na_drop(training_df) # expected to drop 3248 rows, for these rows there were no transactions recorded despite sales present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8972d9-a14e-400f-9f88-9906ed9df471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge training & stores by store_nbr\n",
    "training_df = training_df.join(stores_df, on='store_nbr', how='left')\n",
    "training_df = smart_na_drop(training_df) # expected to drop 0 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0425569-4fdf-48a1-80db-698e8baef4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge training & holidays by date\n",
    "training_df = training_df.join(holidays_df, on='date', how='left')\n",
    "# impute for absent rows\n",
    "training_df = smart_na_drop(training_df) # expected to drop 0 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce71afd-ea08-4a0b-809b-5e0a9ed50b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop 'family', 'id' columns\n",
    "training_df = training_df.drop('family', 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ada4020-ba32-4b51-be4b-c1d0f85003b6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765317330276}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df.printSchema()\n",
    "display(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102a7bc2-5d51-4ca3-93ec-10d0f90bb4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hash with FeatureHasher to create a new column with hashed values for 'store_nbr' column\n",
    "# hash 'state' column\n",
    "holidays_hasher = FeatureHasher(\n",
    "    inputCols=['store_nbr'],\n",
    "    outputCol='hash_storeNbr',\n",
    "    numFeatures=1024\n",
    ")\n",
    "training_df = holidays_hasher.transform(training_df)\n",
    "\n",
    "# drop 'store_nbr' column\n",
    "training_df = training_df.drop('store_nbr')\n",
    "\n",
    "display(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13017c8c-24f5-4c1a-a486-02182d613c1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write Silver tier as Delta table\n",
    "# WARN: will take approx. 2 minutes\n",
    "silver_path = f\"{VOLUME_SILVER_DIR}/training\"\n",
    "training_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{VOLUME_SILVER_DIR}/training\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
