{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95edd353-2518-4745-bb1d-e79b8c9ccbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation for Linear Regression\n",
    "This notebook outlines the data preparation steps for a linear regression analysis. It organizes the workflow into different tiers:\n",
    "\n",
    "### Bronze Tier\n",
    "Tables as they are\n",
    "\n",
    "### Silver Tier\n",
    "The Silver Tier contains curated, cleaned, and joined data.\n",
    "\n",
    "#### Table `encoded_train_df`:  \n",
    "Numerical variables: `total_daily_sales`, `days_since_earliest_date`, `transactions`, `onpromotion`.  \n",
    "\n",
    "Categorical variables: `store_nbr`, `city`, `state`, `type`, `cluster`, `day_of_week`, `day_of_month`, `month`, `year`.  \n",
    "All categorical variables are one-hot encoded using prefix `is_<varname>_` e.g. `is_state__Ohio` (note the double underscore).\n",
    "\n",
    "\n",
    "This tier is designed for analysis and modeling, providing a structured dataset with relevant features for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ab9df2-7ff6-40eb-b71a-30d62d95eb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdfca57-f68b-42cc-8d5d-fd7d8c330067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# raw data\n",
    "VOLUME_BRONZE_DIR = f\"{VOLUME_ROOT_PATH}/bronze\"\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_BRONZE_DIR, VOLUME_SILVER_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106b229a-9aaf-4639-9fa2-ce78ee51c8dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from local volumes\n",
    "bronze_filenames = {\n",
    "    'holidays_events': 'holidays_v1',\n",
    "    'oil': 'oil',\n",
    "    'sample_submission': 'sample_submission',\n",
    "    'stores': 'stores',\n",
    "    'test': 'test',\n",
    "    'train': 'train',\n",
    "    'transactions': 'transactions'\n",
    "}\n",
    "silver_filenames = {\n",
    "    'holidays_events': 'holidays',\n",
    "    'oil': 'oil',\n",
    "    'sample_submission': 'sample_submission',\n",
    "    'stores': 'stores',\n",
    "    'test': 'test',\n",
    "    'train': 'train',\n",
    "    'transactions': 'transactions'\n",
    "}\n",
    "\n",
    "# read from Bronze Tier\n",
    "bstores_df = spark.read.parquet(f\"{VOLUME_BRONZE_DIR}/stores\")\n",
    "btransactions_df = spark.read.parquet(f\"{VOLUME_BRONZE_DIR}/transactions\")\n",
    "btrain_df = spark.read.parquet(f\"{VOLUME_BRONZE_DIR}/train\")\n",
    "btest_df = spark.read.parquet(f\"{VOLUME_BRONZE_DIR}/test\")\n",
    "bholidays_events_df = spark.read.parquet(f\"{VOLUME_BRONZE_DIR}/holidays\")\n",
    "# oil_df = spark.read.parquet(f\"{VOLUME_BRONZE_DIR}/oil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe524d6-19d6-434d-9625-5fdf2cec26c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Tier\n",
    "\n",
    "Produce & persist table:  \n",
    "\n",
    "store_nbr\t|   int  \n",
    "date\t    |   date  \n",
    "id\t        |   int  \n",
    "family\t    |   string  \n",
    "sales\t    |   double  \n",
    "onpromotion\t|   int  \n",
    "transactions|\tint  \n",
    "city\t    |   string  \n",
    "state\t    |   string  \n",
    "type\t    |   string  \n",
    "cluster\t    |   int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa6ced0-fee7-4372-94db-530076b71e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def smart_na_drop(df):\n",
    "    \"\"\"\n",
    "    Drops all rows with any null values in columns.\n",
    "    \"\"\"\n",
    "    before = df.count()\n",
    "    df = df.dropna()\n",
    "    after = df.count()\n",
    "    print(f\"dropped {before - after} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1b4806-b4c8-4fcb-a69b-569b128f091b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "btrain_df = smart_na_drop(btrain_df)\n",
    "btransactions_df = smart_na_drop(btransactions_df)\n",
    "bstores_df = smart_na_drop(bstores_df)\n",
    "bholidays_events_df = smart_na_drop(bholidays_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea5f22c-d906-4b96-a310-9093c2e634e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664309ab-dc6e-4e87-9861-124e51f23d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bholidays_events_df.printSchema()\n",
    "display(bholidays_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a1233c-e40b-4daf-b439-a980da61c814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rows_to_value(df, name):\n",
    "    return [ row[name] for row in df ]\n",
    "\n",
    "display(rows_to_value(bholidays_events_df.select('locale_name').distinct().collect(), 'locale_name'))\n",
    "bholidays_events_df.select('locale').distinct().show()\n",
    "bholidays_events_df.select('type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8028997-4084-4f9b-ba06-b008152a80d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparation of holidays data (holidays_events_df):\n",
    "# 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "#    Identifiable by 'type' = 'Transfer'\n",
    "# 2. Explode nationwade holiday to per state, identifiable by 'locale_name' = 'Ecuador'\n",
    "# 3. Deduplicate dates. This is made under assumption that all the rest of holiday types are actual holidays.\n",
    "# 4. Construct new dataframe with 2 columns: 'date', 'is_holiday' from the holidays df\n",
    "# 5. Write to Bronze tier\n",
    "\n",
    "sholidays_events_df = bholidays_events_df\n",
    "\n",
    "# 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "sholidays_events_df = sholidays_events_df.where(F.col('locale_name') != 'Transfer')\n",
    "\n",
    "# 2. Explode nationwade holiday to per state, identifiable by 'locale_name' = 'Ecuador'\n",
    "# list of states is provided by the stores_df\n",
    "ecuador_states = [ row['state'] for row in bstores_df.select('state').distinct().collect()]\n",
    "\n",
    "# add array with all the states to 'Ecuador' rows\n",
    "sholidays_events_df = sholidays_events_df.withColumn(\n",
    "    'locale_name_array',\n",
    "    F.when(\n",
    "        F.col('locale_name') == 'Ecuador',\n",
    "        F.array([ F.lit(s) for s in ecuador_states ])\n",
    "    ).otherwise(\n",
    "        F.array(F.col('locale_name'))\n",
    "    )\n",
    ")\n",
    "# Explode & \n",
    "# 4. Construct new dataframe with 2 columns: 'date', 'is_holiday' from the holidays df\n",
    "sholidays_events_df = sholidays_events_df.select(\n",
    "    'date',\n",
    "    F.explode('locale_name_array').alias('state'),\n",
    "    F.lit(1).alias('is_holiday') \n",
    ")\n",
    "\n",
    "# 3. Deduplicate rows by leaving unique per date-state\n",
    "sholidays_events_df = sholidays_events_df.dropDuplicates(['date', 'state'])\n",
    "\n",
    "# 5. Write to Silver tier\n",
    "sholidays_events_df.write.mode('overwrite').parquet(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('holidays_events')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1540413b-eef5-4cd5-bd2b-77bb51da8883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sholidays_events_df.printSchema()\n",
    "display(sholidays_events_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29074488-624e-4319-8d9a-df3240298a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63d5e50-5f16-4fb1-9872-6e743f51ec65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "btransactions_df.printSchema()\n",
    "display(btransactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec01076-8d4a-4ce8-9d2c-8b82c8ab24bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "btransactions_df.write.mode('overwrite').parquet(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('transactions')}\")\n",
    "stransactions_df = spark.read.parquet(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('transactions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eae94e9-3251-470c-a642-b9430f3261cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7215771b-88b5-4181-87c0-20e1264c7294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bstores_df.printSchema()\n",
    "display(bstores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ac3ca8-6285-4fc9-8f00-4a2c66024a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. One hot encode categorical columns: city, state, type, cluster\n",
    "bstores_categorical_cols = ['city', 'state', 'type', 'cluster']\n",
    "\n",
    "bstores_indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + '_idx', handleInvalid='keep')\n",
    "    for col in bstores_categorical_cols\n",
    "]\n",
    "\n",
    "bstores_encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[col + '_idx'], \n",
    "        outputCols=['is_' + col],\n",
    "    )\n",
    "    for col in bstores_categorical_cols\n",
    "]\n",
    "\n",
    "bstores_pipeline = Pipeline(stages=bstores_indexers + bstores_encoders)\n",
    "bstores_pipeline_model = bstores_pipeline.fit(bstores_df)\n",
    "bstores_df_encoded = bstores_pipeline_model.transform(bstores_df)\n",
    "display(bstores_df)\n",
    "\n",
    "bstores_ohe_cols = ['is_' + col for col in bstores_categorical_cols]\n",
    "# Get all columns *except* the temporary index columns\n",
    "bstores_df_cols_to_select = [col for col in bstores_df.columns if col not in [f'is_{c}' for c in bstores_categorical_cols]]\n",
    "bstores_df_cols_to_select.extend(bstores_ohe_cols)\n",
    "\n",
    "sstores_df = bstores_df_encoded.select(bstores_df_cols_to_select)\n",
    "\n",
    "display(sstores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88bf511c-aafb-4747-ac77-8eabcc15447d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop original categorical cols\n",
    "sstores_df = sstores_df.drop(*bstores_categorical_cols)\n",
    "\n",
    "display(sstores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02de4bee-d0b2-42f2-9440-4ab330472379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd2edb3-2dcf-4522-8d24-06137c31184e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "917ff48c-70e5-4d37-b031-89ba5fbd5167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# This shows the rows which are dropped in the cell below after merging with transactions\n",
    "test_df = train_df\n",
    "test_df = test_df.join(btransactions_df, on=['date', 'store_nbr'], how='left')\n",
    "test_df = test_df.withColumn(\n",
    "    'transactions',\n",
    "    F.when(F.col('sales') == 0, 0).otherwise(F.col('transactions'))\n",
    ")\n",
    "# show only rows where nulls are present\n",
    "test_df.where(F.col('transactions').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ef1b1f-30c6-43ff-b324-3010d379f72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Merge with transactions data\n",
    "#       .a Fill transactions as 0 when total_daily_sales is 0\n",
    "#       .b Drop rows where any column is null\n",
    "# 2. Merge with stores_df\n",
    "\n",
    "strain_df = btrain_df\n",
    "\n",
    "# 1. Merge with transactions data\n",
    "strain_df = strain_df.join(btransactions_df, on=['date', 'store_nbr'], how='left')\n",
    "strain_df = strain_df.withColumn(\n",
    "    'transactions',\n",
    "    F.when(F.col('sales') == 0, 0).otherwise(F.col('transactions'))\n",
    ")\n",
    "strain_df = smart_na_drop(strain_df) # expected to drop 3248 rows\n",
    "\n",
    "# 2. Merge with stores_df\n",
    "strain_df = strain_df.join(bstores_df, ['store_nbr'], how='left')\n",
    "strain_df = smart_na_drop(strain_df) # expected to drop 0 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158c7375-0e2e-4e1b-9064-527de41ef4de",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764797752466}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema of strain_df\n",
    "strain_df.printSchema()\n",
    "\n",
    "strain_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17753d91-df43-484f-8fba-ff9eefe68c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Add columns day_of_week, day, month, year\n",
    "# 2. Add column days_since_earliest_date as number of days since earliest date\n",
    "# 3. Drop date column\n",
    "\n",
    "# 1. Add columns day_of_week, day, month, year\n",
    "strain_df = strain_df.withColumn('day_of_week', F.dayofweek(F.col('date')))\n",
    "strain_df = strain_df.withColumn('day_of_month', F.dayofmonth(F.col('date')))\n",
    "strain_df = strain_df.withColumn('month', F.month(F.col('date')))\n",
    "strain_df = strain_df.withColumn('year', F.year(F.col('date')))\n",
    "\n",
    "# 2. Add column time_since_earliest_date\n",
    "earliest_date = strain_df.select(F.min('date')).collect()[0][0] # Normally is 2013-01-01\n",
    "strain_df = strain_df.withColumn(\n",
    "    'days_since_earliest_date',\n",
    "    F.datediff(F.col('date'), F.lit(earliest_date))\n",
    ")\n",
    "\n",
    "# 3. Drop date column\n",
    "strain_df = strain_df.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de11008-82a6-4d4a-9849-be814d51f359",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764798167265}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparation of the data for Logistic Regression\n",
    "# 1. One-hot encode all categorical\n",
    "#  variables: store_nbr, city, state, type, cluster, day_of_week, day_of_month, month, year\n",
    "\n",
    "# 1. One-hot encode all categorical variables: store_nbr, city, state, type, cluster, day_of_week, day_of_month, month, year\n",
    "for colname in ['store_nbr', 'city', 'state', 'type', 'cluster', 'day_of_week', 'day_of_month', 'month', 'year']:\n",
    "    setrain_df = pd.get_dummies(\n",
    "        setrain_df,\n",
    "        columns=[colname],\n",
    "        dtype=int,\n",
    "        prefix=f'is_{colname}_'\n",
    "    )\n",
    "display(setrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9e8e19-38de-44e8-a16d-dbe973860e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert setrain_df back to spark dataframe\n",
    "# 2. Write setrain_df to silver table\n",
    "\n",
    "# 1. Convert setrain_df back to spark dataframe\n",
    "setrain_df = spark.createDataFrame(setrain_df)\n",
    "\n",
    "# 2. Write setrain_df to silver table\n",
    "setrain_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_SILVER_DIR}/encoded_train_df\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "old_silver_tier",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
