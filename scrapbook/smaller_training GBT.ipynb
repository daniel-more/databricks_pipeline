{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc8d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/24 01:56:16 WARN Utils: Your hostname, daniels-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.88.117 instead (on interface en0)\n",
      "25/12/24 01:56:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/24 01:56:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark restarted with 12GB driver and executor memory\n",
      "+----+------+------+--------------------+----------+---------+-----+-----------+-------+----------+----------+--------------------+----------------+--------------+---------------+--------------+-----------+------------+-----+----+-------------+\n",
      "|type| state|  city|              family|      date|store_nbr|sales|onpromotion|cluster|is_holiday|dcoilwtico|       hash_features|strIndxer_family|strIndxer_city|strIndxer_state|strIndxer_type|day_of_week|day_of_month|month|year|is_salary_day|\n",
      "+----+------+------+--------------------+----------+---------+-----+-----------+-------+----------+----------+--------------------+----------------+--------------+---------------+--------------+-----------+------------+-----+----+-------------+\n",
      "|   D| Azuay|Cuenca|SCHOOL AND OFFICE...|2013-01-01|       37|  0.0|          0|      2|         1|     93.14|(1024,[124,560,79...|              27|            14|             15|             4|          3|           1|    1|2013|            0|\n",
      "|   C|Guayas|Playas|SCHOOL AND OFFICE...|2013-01-02|       35|  0.0|          0|      3|         0|     93.14|(1024,[191,373,47...|              27|             9|              7|             1|          4|           2|    1|2013|            0|\n",
      "+----+------+------+--------------------+----------+---------+-----+-----------+-------+----------+----------+--------------------+----------------+--------------+---------------+--------------+-----------+------------+-----+----+-------------+\n",
      "only showing top 2 rows\n",
      "Total rows: 3000888\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Stop current Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Give Spark way more memory since you have 32GB RAM available\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TimeSeriesForecast\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark restarted with 12GB driver and executor memory\")\n",
    "\n",
    "# Read data\n",
    "df = spark.read.parquet('../notebooks/data/train.parquet')\n",
    "df.show(2)\n",
    "print(f\"Total rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7772b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable rows: 2,369,333\n",
      "\n",
      "Verifying lag features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sales_lag_7: 15 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sales_lag_14: 5,849 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sales_rolling_mean_7: 0 nulls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Start with RAW data\n",
    "df_raw = df\n",
    "\n",
    "# 2. CREATE LAG FEATURES FIRST (on full data!)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_store_family = Window.partitionBy(\"store_nbr\", \"family\").orderBy(\"date\")\n",
    "window_7d = Window.partitionBy(\"store_nbr\", \"family\").orderBy(\"date\").rowsBetween(-7, -1)\n",
    "window_14d = Window.partitionBy(\"store_nbr\", \"family\").orderBy(\"date\").rowsBetween(-14, -1)\n",
    "\n",
    "print(\"Creating lag features...\")\n",
    "df_with_lags = df_raw \\\n",
    "    .withColumn(\"sales_lag_7\", F.lag(\"sales\", 7).over(window_store_family)) \\\n",
    "    .withColumn(\"sales_lag_14\", F.lag(\"sales\", 14).over(window_store_family)) \\\n",
    "    .withColumn(\"sales_lag_30\", F.lag(\"sales\", 30).over(window_store_family)) \\\n",
    "    .withColumn(\"sales_rolling_mean_7\", F.avg(\"sales\").over(window_7d)) \\\n",
    "    .withColumn(\"sales_rolling_mean_14\", F.avg(\"sales\").over(window_14d)) \\\n",
    "    .withColumn(\"sales_rolling_std_7\", F.stddev(\"sales\").over(window_7d)) \\\n",
    "    .withColumn(\"promo_rolling_sum_7\", F.sum(\"onpromotion\").over(window_7d))\n",
    "\n",
    "# Add date features\n",
    "df_with_lags = df_with_lags \\\n",
    "    .withColumn(\"week_of_year\", F.weekofyear(\"date\")) \\\n",
    "    .withColumn(\"quarter\", F.quarter(\"date\")) \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([5, 6]), 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_month_start\", F.when(F.dayofmonth(\"date\") <= 5, 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_month_end\", F.when(F.dayofmonth(\"date\") >= 25, 1).otherwise(0))\n",
    "\n",
    "# 3. NOW find stable periods\n",
    "window_14d_density = Window.partitionBy(\"store_nbr\", \"family\").orderBy(\"date\").rowsBetween(-13, 0)\n",
    "\n",
    "df_with_density = df_with_lags.withColumn(\n",
    "    \"sales_days_in_window\",\n",
    "    F.sum(F.when(F.col(\"sales\") > 0, 1).otherwise(0)).over(window_14d_density)\n",
    ")\n",
    "\n",
    "stable_starts = df_with_density.filter(F.col(\"sales_days_in_window\") >= 7) \\\n",
    "    .groupBy(\"store_nbr\", \"family\") \\\n",
    "    .agg(F.min(\"date\").alias(\"stable_start_date\"))\n",
    "\n",
    "# 4. Filter to stable periods\n",
    "df_with_stable = df_with_lags.join(\n",
    "    stable_starts.select(\"store_nbr\", \"family\", \"stable_start_date\"), \n",
    "    on=[\"store_nbr\", \"family\"], \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "df_stable = df_with_stable.filter(F.col(\"date\") >= F.col(\"stable_start_date\"))\n",
    "\n",
    "# 5. Require sufficient data\n",
    "sufficient_data = df_stable.groupBy(\"store_nbr\", \"family\").agg(\n",
    "    F.countDistinct(\"date\").alias(\"stable_days\"),\n",
    "    F.sum(\"sales\").alias(\"total_sales\")\n",
    ").filter(\n",
    "    (F.col(\"stable_days\") >= 365) &\n",
    "    (F.col(\"total_sales\") >= 0)\n",
    ")\n",
    "\n",
    "df_trainable = df_stable.join(\n",
    "    sufficient_data.select(\"store_nbr\", \"family\"),\n",
    "    on=[\"store_nbr\", \"family\"],\n",
    "    how=\"inner\"\n",
    ").drop(\"stable_start_date\", \"sales_days_in_window\")\n",
    "\n",
    "print(f\"Trainable rows: {df_trainable.count():,}\")\n",
    "\n",
    "# 6. Verify lag features exist\n",
    "print(\"\\nVerifying lag features:\")\n",
    "for col in ['sales_lag_7', 'sales_lag_14', 'sales_rolling_mean_7']:\n",
    "    if col in df_trainable.columns:\n",
    "        null_count = df_trainable.filter(F.col(col).isNull()).count()\n",
    "        print(f\"✓ {col}: {null_count:,} nulls\")\n",
    "    else:\n",
    "        print(f\"❌ {col}: MISSING!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f472f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/24 01:56:44 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+----+---------+-----+----------+-----+-----------+-------+----------+----------+--------------------+----------------+--------------+---------------+--------------+-----------+------------+-----+----+-------------+-----------+------------+------------+--------------------+---------------------+-------------------+-------------------+------------+-------+----------+--------------+------------+\n",
      "|store_nbr|            family|type|    state| city|      date|sales|onpromotion|cluster|is_holiday|dcoilwtico|       hash_features|strIndxer_family|strIndxer_city|strIndxer_state|strIndxer_type|day_of_week|day_of_month|month|year|is_salary_day|sales_lag_7|sales_lag_14|sales_lag_30|sales_rolling_mean_7|sales_rolling_mean_14|sales_rolling_std_7|promo_rolling_sum_7|week_of_year|quarter|is_weekend|is_month_start|is_month_end|\n",
      "+---------+------------------+----+---------+-----+----------+-----+-----------+-------+----------+----------+--------------------+----------------+--------------+---------------+--------------+-----------+------------+-----+----+-------------+-----------+------------+------------+--------------------+---------------------+-------------------+-------------------+------------+-------+----------+--------------+------------+\n",
      "|        4|HOME AND KITCHEN I|   D|Pichincha|Quito|2014-01-08| 23.0|          0|      9|         0|      91.9|(1024,[21,384,560...|               1|            18|             14|             4|          4|           8|    1|2014|            0|        0.0|         0.0|         0.0|  22.571428571428573|   11.285714285714286| 15.371587890836595|                  0|           2|      1|         0|             0|           0|\n",
      "|        4|HOME AND KITCHEN I|   D|Pichincha|Quito|2014-01-09| 26.0|          0|      9|         0|     91.36|(1024,[21,384,560...|               1|            18|             14|             4|          5|           9|    1|2014|            0|       28.0|         0.0|         0.0|  25.857142857142858|   12.928571428571429| 11.781745363464777|                  0|           2|      1|         1|             0|           0|\n",
      "+---------+------------------+----+---------+-----+----------+-----+-----------+-------+----------+----------+--------------------+----------------+--------------+---------------+--------------+-----------+------------+-----+----+-------------+-----------+------------+------------+--------------------+---------------------+-------------------+-------------------+------------+-------+----------+--------------+------------+\n",
      "only showing top 2 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['store_nbr',\n",
       " 'family',\n",
       " 'type',\n",
       " 'state',\n",
       " 'city',\n",
       " 'date',\n",
       " 'sales',\n",
       " 'onpromotion',\n",
       " 'cluster',\n",
       " 'is_holiday',\n",
       " 'dcoilwtico',\n",
       " 'hash_features',\n",
       " 'strIndxer_family',\n",
       " 'strIndxer_city',\n",
       " 'strIndxer_state',\n",
       " 'strIndxer_type',\n",
       " 'day_of_week',\n",
       " 'day_of_month',\n",
       " 'month',\n",
       " 'year',\n",
       " 'is_salary_day',\n",
       " 'sales_lag_7',\n",
       " 'sales_lag_14',\n",
       " 'sales_lag_30',\n",
       " 'sales_rolling_mean_7',\n",
       " 'sales_rolling_mean_14',\n",
       " 'sales_rolling_std_7',\n",
       " 'promo_rolling_sum_7',\n",
       " 'week_of_year',\n",
       " 'quarter',\n",
       " 'is_weekend',\n",
       " 'is_month_start',\n",
       " 'is_month_end']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_trainable.filter(F.col('family').isin(['HOME AND KITCHEN I', 'PRODUCE']))\n",
    "df.show(2)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2110ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 2369333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean sample size: 2363484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------------+-----------+----------+-----------+-----+----+-----------+------------+--------------------+-------------------+------------+----------+--------------------+\n",
      "|      date| sales|strIndxer_family|onpromotion|is_holiday|day_of_week|month|year|sales_lag_7|sales_lag_14|sales_rolling_mean_7|promo_rolling_sum_7|week_of_year|is_weekend|            features|\n",
      "+----------+------+----------------+-----------+----------+-----------+-----+----+-----------+------------+--------------------+-------------------+------------+----------+--------------------+\n",
      "|2015-12-13| 490.0|              26|          0|         0|          1|   12|2015|      514.0|       419.0|   316.2857142857143|                  4|          50|         0|[26.0,0.0,0.0,1.0...|\n",
      "|2015-05-28|   2.0|              11|          0|         0|          5|    5|2015|        1.0|         1.0|  0.7142857142857143|                  0|          22|         1|[11.0,0.0,0.0,5.0...|\n",
      "|2015-02-27|1067.0|               3|          0|         0|          6|    2|2015|     1246.0|      1079.0|  1061.5714285714287|                  3|           9|         1|[3.0,0.0,0.0,6.0,...|\n",
      "|2014-10-11|   9.0|              23|          0|         0|          7|   10|2014|       10.0|        10.0|   7.142857142857143|                  0|          41|         0|[23.0,0.0,0.0,7.0...|\n",
      "|2015-07-01|6384.0|              20|         14|         0|          4|    7|2015|     4757.0|      5107.0|   5496.857142857143|                 76|          27|         0|[20.0,14.0,0.0,4....|\n",
      "+----------+------+----------------+-----------+----------+-----------+-----+----+-----------+------------+--------------------+-------------------+------------+----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2013-01-15 to 2017-08-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1890787, Test: 436850\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results on 10% Sample ===\n",
      "RMSE: 482.02\n",
      "MAE: 121.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 303:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+\n",
      "|      date|    sales|        prediction|\n",
      "+----------+---------+------------------+\n",
      "|2013-01-15|   66.642|40.401009108090335|\n",
      "|2013-01-15|     95.0| 75.37626932353274|\n",
      "|2013-01-15|    141.0| 97.63842771795456|\n",
      "|2013-01-15|    156.0|100.06129060578576|\n",
      "|2013-01-16|  489.326|511.82928773379325|\n",
      "|2013-01-16|502.45798|1631.8953649837717|\n",
      "|2013-01-18|      0.0| 26.02617578820253|\n",
      "|2013-01-19|   1310.0|1088.3451383793213|\n",
      "|2013-01-21|      6.0| 26.02617578820253|\n",
      "|2013-01-21|  184.206|  135.054084640947|\n",
      "+----------+---------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# IMPORTANT: Work with a manageable sample first\n",
    "df_sample = df_trainable.sample(fraction=1.0, seed=42)  # Use 10% of data\n",
    "print(f\"Sample size: {df_sample.count()}\")\n",
    "\n",
    "# Define feature columns (only the most important ones to reduce memory)\n",
    "feature_cols = ['strIndxer_family',\n",
    "    'onpromotion', 'is_holiday', 'day_of_week', 'month', 'year',\n",
    "    'sales_lag_7', 'sales_lag_14', 'sales_rolling_mean_7',\n",
    "    'promo_rolling_sum_7', 'week_of_year', 'is_weekend'\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "feature_cols = [col for col in feature_cols if col in df_sample.columns]\n",
    "\n",
    "# Remove nulls\n",
    "df_clean = df_sample.select(['date', 'sales'] + feature_cols).na.drop()\n",
    "\n",
    "# Repartition to reduce memory pressure\n",
    "df_clean = df_clean.repartition(4)\n",
    "\n",
    "print(f\"Clean sample size: {df_clean.count()}\")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_clean)\n",
    "df_assembled.show(5)\n",
    "\n",
    "# Get date range\n",
    "dates = df_assembled.agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "print(f\"Date range: {dates[0]} to {dates[1]}\")\n",
    "\n",
    "# Simple date split\n",
    "train_df = df_assembled.limit(int(df_assembled.count() * 0.8))\n",
    "test_df = df_assembled.subtract(train_df)\n",
    "\n",
    "print(f\"Train: {train_df.count()}, Test: {test_df.count()}\")\n",
    "\n",
    "# Use Random Forest (lighter than GBT)\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"sales\",\n",
    "    numTrees=20,  # Reduced from default\n",
    "    maxDepth=5,   # Reduced depth\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "model = rf.fit(train_df)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "\n",
    "print(f\"\\n=== Results on 10% Sample ===\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "predictions.select(\"date\", \"sales\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb1ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import avg, sqrt, sum as spark_sum, count, abs as spark_abs, col\n",
    "\n",
    "def evaluate_regression_model(predictions_df, label_col='sales', prediction_col='prediction', verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate regression model predictions with comprehensive metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_df : pyspark.sql.DataFrame\n",
    "        DataFrame containing actual and predicted values\n",
    "    label_col : str\n",
    "        Name of the column with actual values (default: 'sales')\n",
    "    prediction_col : str\n",
    "        Name of the column with predicted values (default: 'prediction')\n",
    "    verbose : bool\n",
    "        Whether to print metrics (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Overall metrics using RegressionEvaluator\n",
    "    evaluator_rmse = RegressionEvaluator(\n",
    "        labelCol=label_col,\n",
    "        predictionCol=prediction_col,\n",
    "        metricName='rmse'\n",
    "    )\n",
    "    \n",
    "    evaluator_mae = RegressionEvaluator(\n",
    "        labelCol=label_col,\n",
    "        predictionCol=prediction_col,\n",
    "        metricName='mae'\n",
    "    )\n",
    "    \n",
    "    evaluator_r2 = RegressionEvaluator(\n",
    "        labelCol=label_col,\n",
    "        predictionCol=prediction_col,\n",
    "        metricName='r2'\n",
    "    )\n",
    "    \n",
    "    evaluator_mse = RegressionEvaluator(\n",
    "        labelCol=label_col,\n",
    "        predictionCol=prediction_col,\n",
    "        metricName='mse'\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = evaluator_rmse.evaluate(predictions_df)\n",
    "    mae = evaluator_mae.evaluate(predictions_df)\n",
    "    r2 = evaluator_r2.evaluate(predictions_df)\n",
    "    mse = evaluator_mse.evaluate(predictions_df)\n",
    "    \n",
    "    # Additional custom metrics\n",
    "    stats = predictions_df.agg(\n",
    "        count(label_col).alias('count'),\n",
    "        avg(label_col).alias('mean_actual'),\n",
    "        avg(prediction_col).alias('mean_predicted'),\n",
    "        sqrt(avg((col(label_col) - col(prediction_col)) ** 2)).alias('rmse_check'),\n",
    "        avg(spark_abs(col(label_col) - col(prediction_col))).alias('mae_check'),\n",
    "        avg(spark_abs((col(label_col) - col(prediction_col)) / col(label_col)) * 100).alias('mape')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Compile results\n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'mape': stats['mape'],\n",
    "        'count': stats['count'],\n",
    "        'mean_actual': stats['mean_actual'],\n",
    "        'mean_predicted': stats['mean_predicted']\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"REGRESSION MODEL EVALUATION METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Sample Count:        {metrics['count']:,}\")\n",
    "        print(f\"Mean Actual:         {metrics['mean_actual']:.2f}\")\n",
    "        print(f\"Mean Predicted:      {metrics['mean_predicted']:.2f}\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"RMSE:                {metrics['rmse']:.4f}\")\n",
    "        print(f\"MAE:                 {metrics['mae']:.4f}\")\n",
    "        print(f\"MSE:                 {metrics['mse']:.4f}\")\n",
    "        print(f\"R² Score:            {metrics['r2']:.4f}\")\n",
    "        print(f\"MAPE:                {metrics['mape']:.2f}%\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_by_group(predictions_df, group_cols, label_col='sales', prediction_col='prediction', top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate metrics for each group (e.g., by store_nbr and family).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_df : pyspark.sql.DataFrame\n",
    "        DataFrame containing actual and predicted values\n",
    "    group_cols : list or str\n",
    "        Column(s) to group by (e.g., ['store_nbr', 'family'])\n",
    "    label_col : str\n",
    "        Name of the column with actual values (default: 'sales')\n",
    "    prediction_col : str\n",
    "        Name of the column with predicted values (default: 'prediction')\n",
    "    top_n : int\n",
    "        Number of top/bottom groups to display (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pyspark.sql.DataFrame : Metrics by group\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols = [group_cols]\n",
    "    \n",
    "    metrics_by_group = predictions_df.groupBy(*group_cols).agg(\n",
    "        count(label_col).alias('sample_count'),\n",
    "        avg(label_col).alias('avg_actual'),\n",
    "        avg(prediction_col).alias('avg_predicted'),\n",
    "        sqrt(avg((col(label_col) - col(prediction_col)) ** 2)).alias('rmse'),\n",
    "        avg(spark_abs(col(label_col) - col(prediction_col))).alias('mae'),\n",
    "        avg(spark_abs((col(label_col) - col(prediction_col)) / col(label_col)) * 100).alias('mape')\n",
    "    ).orderBy('rmse', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRICS BY GROUP: {', '.join(group_cols)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nTop {top_n} groups with HIGHEST error (RMSE):\")\n",
    "    print(\"-\"*80)\n",
    "    metrics_by_group.show(top_n, truncate=False)\n",
    "    \n",
    "    print(f\"\\nTop {top_n} groups with LOWEST error (RMSE):\")\n",
    "    print(\"-\"*80)\n",
    "    metrics_by_group.orderBy('rmse', ascending=True).show(top_n, truncate=False)\n",
    "    \n",
    "    return metrics_by_group\n",
    "\n",
    "\n",
    "def compare_models(predictions_dfs, model_names, label_col='sales', prediction_col='prediction'):\n",
    "    \"\"\"\n",
    "    Compare metrics across multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_dfs : list\n",
    "        List of prediction DataFrames from different models\n",
    "    model_names : list\n",
    "        List of model names corresponding to each DataFrame\n",
    "    label_col : str\n",
    "        Name of the column with actual values (default: 'sales')\n",
    "    prediction_col : str\n",
    "        Name of the column with predicted values (default: 'prediction')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pyspark.sql.DataFrame : Comparison table of all models\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, pred_df in zip(model_names, predictions_dfs):\n",
    "        metrics = evaluate_regression_model(pred_df, label_col, prediction_col, verbose=False)\n",
    "        metrics['model_name'] = name\n",
    "        results.append(metrics)\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    comparison_df = spark.createDataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    comparison_df.select(\n",
    "        'model_name', 'rmse', 'mae', 'r2', 'mape', 'count'\n",
    "    ).show(truncate=False)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def plot_predictions_sample(predictions_df, label_col='sales', prediction_col='prediction', sample_size=1000):\n",
    "    \"\"\"\n",
    "    Display a sample of actual vs predicted values for quick inspection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions_df : pyspark.sql.DataFrame\n",
    "        DataFrame containing actual and predicted values\n",
    "    label_col : str\n",
    "        Name of the column with actual values (default: 'sales')\n",
    "    prediction_col : str\n",
    "        Name of the column with predicted values (default: 'prediction')\n",
    "    sample_size : int\n",
    "        Number of samples to display (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pyspark.sql.DataFrame : Sample of predictions with error metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_df = predictions_df.select(\n",
    "        '*',\n",
    "        (col(prediction_col) - col(label_col)).alias('error'),\n",
    "        spark_abs(col(prediction_col) - col(label_col)).alias('abs_error'),\n",
    "        (spark_abs((col(label_col) - col(prediction_col)) / col(label_col)) * 100).alias('pct_error')\n",
    "    ).limit(sample_size)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLE PREDICTIONS (First {sample_size} rows)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    sample_df.select(\n",
    "        label_col, prediction_col, 'error', 'abs_error', 'pct_error'\n",
    "    ).show(20)\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cee304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/24 02:00:36 ERROR Executor: Exception in task 0.0 in stage 459.0 (TID 1056)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 ERROR Executor: Exception in task 2.0 in stage 459.0 (TID 1058)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 ERROR Executor: Exception in task 1.0 in stage 459.0 (TID 1057)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 ERROR Executor: Exception in task 5.0 in stage 459.0 (TID 1061)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 ERROR Executor: Exception in task 7.0 in stage 459.0 (TID 1063)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 ERROR Executor: Exception in task 6.0 in stage 459.0 (TID 1062)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 WARN TaskSetManager: Lost task 0.0 in stage 459.0 (TID 1056) (192.168.88.117 executor driver): org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/12/24 02:00:36 ERROR TaskSetManager: Task 0 in stage 459.0 failed 1 times; aborting job\n",
      "25/12/24 02:00:36 ERROR Executor: Exception in task 4.0 in stage 459.0 (TID 1060)\n",
      "org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      "\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/24 02:00:36 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null\n",
      "25/12/24 02:00:36 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null\n",
      "25/12/24 02:00:36 WARN TaskSetManager: Lost task 3.0 in stage 459.0 (TID 1059) (192.168.88.117 executor driver): TaskKilled (Stage cancelled: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      ")\n",
      "25/12/24 02:00:36 WARN TaskSetManager: Lost task 8.0 in stage 459.0 (TID 1064) (192.168.88.117 executor driver): TaskKilled (Stage cancelled: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      ")\n",
      "25/12/24 02:00:36 WARN TaskSetManager: Lost task 9.0 in stage 459.0 (TID 1065) (192.168.88.117 executor driver): TaskKilled (Stage cancelled: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n",
      "== DataFrame ==\n",
      "\"__truediv__\" was called from\n",
      "line 62 in cell [9]\n",
      ")\n",
      "{\"ts\": \"2025-12-24 02:00:36.733\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \\\"spark.sql.ansi.enabled\\\" to \\\"false\\\" to bypass this error. SQLSTATE: 22012\", \"context\": {\"file\": \"line 62 in cell [9]\", \"line\": \"\", \"fragment\": \"__truediv__\", \"errorClass\": \"DIVIDE_BY_ZERO\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o1003.collectToPython.\\n: org.apache.spark.SparkArithmeticException: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \\\"spark.sql.ansi.enabled\\\" to \\\"false\\\" to bypass this error. SQLSTATE: 22012\\n== DataFrame ==\\n\\\"__truediv__\\\" was called from\\nline 62 in cell [9]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.divideByZeroError(QueryExecutionErrors.scala:205)\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors.divideByZeroError(QueryExecutionErrors.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_subExpr_4$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doConsume_1$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithKeysOutput_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage31.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:153)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"263\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/daniel/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages/py4j/protocol.py\", \"line\": \"326\"}]}}\n"
     ]
    },
    {
     "ename": "ArithmeticException",
     "evalue": "[DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n== DataFrame ==\n\"__truediv__\" was called from\nline 62 in cell [9]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArithmeticException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example 1: Evaluate overall model performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_regression_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example 2: Evaluate by store and family\u001b[39;00m\n\u001b[1;32m      5\u001b[0m group_metrics \u001b[38;5;241m=\u001b[39m evaluate_by_group(\n\u001b[1;32m      6\u001b[0m     predictions, \n\u001b[1;32m      7\u001b[0m     group_cols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_nbr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m, in \u001b[0;36mevaluate_regression_model\u001b[0;34m(predictions_df, label_col, prediction_col, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m mse \u001b[38;5;241m=\u001b[39m evaluator_mse\u001b[38;5;241m.\u001b[39mevaluate(predictions_df)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Additional custom metrics\u001b[39;00m\n\u001b[1;32m     56\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_actual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_col\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_predicted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse_check\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_abs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmae_check\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_abs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Compile results\u001b[39;00m\n\u001b[1;32m     66\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m: mae,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_predicted\u001b[39m\u001b[38;5;124m'\u001b[39m: stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_predicted\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 443\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/databricks_pipeline/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    265\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mArithmeticException\u001b[0m: [DIVIDE_BY_ZERO] Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22012\n== DataFrame ==\n\"__truediv__\" was called from\nline 62 in cell [9]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Evaluate overall model performance\n",
    "metrics = evaluate_regression_model(predictions)\n",
    "\n",
    "# Example 2: Evaluate by store and family\n",
    "group_metrics = evaluate_by_group(\n",
    "    predictions, \n",
    "    group_cols=['store_nbr', 'family'],\n",
    "    top_n=15\n",
    ")\n",
    "\n",
    "# Example 3: Evaluate by family only\n",
    "family_metrics = evaluate_by_group(\n",
    "    predictions,\n",
    "    group_cols='family'\n",
    ")\n",
    "\n",
    "# Example 4: Compare multiple models\n",
    "gbt_predictions = results_gbt['predictions']\n",
    "rf_predictions = results_rf['predictions']\n",
    "lr_predictions = results_lr['predictions']\n",
    "\n",
    "comparison = compare_models(\n",
    "    predictions_dfs=[gbt_predictions, rf_predictions, lr_predictions],\n",
    "    model_names=['GBT', 'Random Forest', 'Linear Regression']\n",
    ")\n",
    "\n",
    "# Example 5: Quick inspection of predictions\n",
    "sample = plot_predictions_sample(predictions, sample_size=500)\n",
    "\n",
    "# Example 6: Silent evaluation (no printing)\n",
    "metrics = evaluate_regression_model(predictions, verbose=False)\n",
    "print(f\"Model RMSE: {metrics['rmse']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001daab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ff4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe0093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ba032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GRANULARITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Total rows\n",
    "total_rows = df.count()\n",
    "print(f\"\\nTotal rows: {total_rows:,}\")\n",
    "\n",
    "# Test different combinations\n",
    "print(\"\\nTesting unique combinations:\")\n",
    "\n",
    "# 1. Date level\n",
    "date_count = df.select(\"date\").distinct().count()\n",
    "print(f\"  Unique dates: {date_count:,}\")\n",
    "\n",
    "# 2. Date + Store\n",
    "date_store = df.select(\"date\", \"store_nbr\").distinct().count()\n",
    "print(f\"  Unique (date, store_nbr): {date_store:,}\")\n",
    "\n",
    "# 3. Date + Family\n",
    "date_family = df.select(\"date\", \"family\").distinct().count()\n",
    "print(f\"  Unique (date, family): {date_family:,}\")\n",
    "\n",
    "# 4. Date + Store + Family (likely the grain)\n",
    "date_store_family = df.select(\"date\", \"store_nbr\", \"family\").distinct().count()\n",
    "print(f\"  Unique (date, store_nbr, family): {date_store_family:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Determine granularity\n",
    "if date_store_family == total_rows:\n",
    "    print(\"✓ GRANULARITY: One row per (date, store_nbr, family)\")\n",
    "    print(\"  This means: Daily sales for each product family at each store\")\n",
    "else:\n",
    "    print(f\"⚠️  DUPLICATES DETECTED!\")\n",
    "    print(f\"  Expected unique combinations: {date_store_family:,}\")\n",
    "    print(f\"  Actual rows: {total_rows:,}\")\n",
    "    print(f\"  Duplicate rows: {total_rows - date_store_family:,}\")\n",
    "    \n",
    "    # Show example duplicates\n",
    "    print(\"\\nSample duplicates:\")\n",
    "    df.groupBy(\"date\", \"store_nbr\", \"family\").count() \\\n",
    "      .filter(F.col(\"count\") > 1) \\\n",
    "      .orderBy(F.desc(\"count\")) \\\n",
    "      .show(5)\n",
    "\n",
    "# Show store attributes relationship\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STORE ATTRIBUTES (should be many-to-one with store_nbr):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "store_attrs = df.select(\"store_nbr\", \"type\", \"state\", \"city\").distinct()\n",
    "store_count = df.select(\"store_nbr\").distinct().count()\n",
    "store_attrs_count = store_attrs.count()\n",
    "\n",
    "print(f\"Unique stores: {store_count}\")\n",
    "print(f\"Unique (store, type, state, city) combos: {store_attrs_count}\")\n",
    "\n",
    "if store_count == store_attrs_count:\n",
    "    print(\"✓ Each store has one unique (type, state, city)\")\n",
    "else:\n",
    "    print(\"⚠️  Some stores have multiple type/state/city values\")\n",
    "\n",
    "# Sample data structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA:\")\n",
    "print(\"=\"*60)\n",
    "df.select(\"date\", \"store_nbr\", \"family\", \"sales\", \"type\", \"city\", \"state\", \"cluster\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Feature columns (excluding family since we're grouping by it)\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    " 'onpromotion',\n",
    " 'cluster',\n",
    " 'is_holiday',\n",
    " 'dcoilwtico',\n",
    "#  'hash_features',\n",
    " 'strIndxer_family',\n",
    " 'strIndxer_city',\n",
    " 'strIndxer_state',\n",
    " 'strIndxer_type',\n",
    " 'day_of_week',\n",
    " 'day_of_month',\n",
    " 'month',\n",
    " 'year',\n",
    " 'is_salary_day',\n",
    " 'sales_lag_7',\n",
    " 'sales_lag_14',\n",
    " 'sales_lag_30',\n",
    " 'sales_rolling_mean_7',\n",
    " 'sales_rolling_mean_14',\n",
    " 'sales_rolling_std_7',\n",
    " 'promo_rolling_sum_7',\n",
    " 'week_of_year',\n",
    " 'quarter',\n",
    " 'is_weekend',\n",
    " 'is_month_start',\n",
    " 'is_month_end']\n",
    "\n",
    "# feature_cols = [\n",
    "#     'onpromotion', 'is_holiday', 'day_of_week', 'month', 'year',\n",
    "#     'sales_lag_7', 'sales_lag_14', 'sales_rolling_mean_7',\n",
    "#     'promo_rolling_sum_7', 'week_of_year', 'is_weekend',\n",
    "#     'strIndxer_city', 'strIndxer_state', 'strIndxer_type', 'cluster'\n",
    "#     # Note: NOT including strIndxer_family since we're training per family\n",
    "# ]\n",
    "\n",
    "feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "\n",
    "# Get unique families\n",
    "families = df.select(\"family\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(f\"Training models for {len(families)} families\")\n",
    "\n",
    "# Dictionary to store models and metrics\n",
    "models = {}\n",
    "metrics = []\n",
    "\n",
    "# Get date range for splitting\n",
    "dates = df.agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "split_date = dates[0] + (dates[1] - dates[0]) * 0.8\n",
    "\n",
    "for family in families:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training model for: {family}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Filter data for this family\n",
    "    df_family = df.filter(F.col(\"family\") == family)\n",
    "    \n",
    "    # Check if sufficient data\n",
    "    count = df_family.count()\n",
    "    if count < 1000:  # Skip if too little data\n",
    "        print(f\"Skipping {family} - insufficient data ({count} rows)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Rows: {count}\")\n",
    "    \n",
    "    # Clean and prepare features\n",
    "    df_clean = df_family.select(['date', 'sales'] + feature_cols).na.drop()\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    df_assembled = assembler.transform(df_clean)\n",
    "    \n",
    "    # Time-based split\n",
    "    train_df = df_assembled.filter(F.col(\"date\") < split_date).cache()\n",
    "    test_df = df_assembled.filter(F.col(\"date\") >= split_date).cache()\n",
    "    \n",
    "    train_count = train_df.count()\n",
    "    test_count = test_df.count()\n",
    "    print(f\"Train: {train_count}, Test: {test_count}\")\n",
    "    \n",
    "    if train_count < 100 or test_count < 10:\n",
    "        print(f\"Skipping {family} - insufficient train/test data\")\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        continue\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"sales\",\n",
    "        numTrees=30,\n",
    "        maxDepth=6,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model = rf.fit(train_df)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.transform(test_df)\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluator_rmse = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        evaluator_mae = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        evaluator_r2 = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        \n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        mae = evaluator_mae.evaluate(predictions)\n",
    "        r2 = evaluator_r2.evaluate(predictions)\n",
    "        \n",
    "        print(f\"RMSE: {rmse:.2f}, MAE: {mae:.2f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        models[family] = model\n",
    "        metrics.append({\n",
    "            'family': family,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'train_rows': train_count,\n",
    "            'test_rows': test_count\n",
    "        })\n",
    "        \n",
    "        # Save model\n",
    "        # model.write().overwrite().save(f\"/Volumes/portfolio_catalog/databricks_pipeline/models/rf_sales_{family.replace(' ', '_')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for {family}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9aa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary of all models\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"SUMMARY OF ALL MODELS\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(metrics).sort_values('r2')\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage RMSE: {metrics_df['rmse'].mean():.2f}\")\n",
    "print(f\"Average MAE: {metrics_df['mae'].mean():.2f}\")\n",
    "print(f\"Average R²: {metrics_df['r2'].mean():.4f}\")\n",
    "    \n",
    "# Save metrics\n",
    "metrics_df.to_csv('./model_metrics_families_all_periods.csv', index=False)\n",
    "print(f\"\\n✓ Metrics saved to ./model_metrics_families_all_periods.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.filter(F.col('family') == 'HOME AND KITCHEN I').groupBy('date', 'store_nbr', 'family').agg(F.sum('sales').alias('sales')).toPandas()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.lineplot(data=dfx, x='date', y='sales', hue='store_nbr', alpha=0.5)\n",
    "plt.title('HOME AND KITCHEN I');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.filter(F.col('family') == 'PRODUCE').groupBy('date', 'store_nbr', 'family').agg(F.sum('sales').alias('sales')).toPandas()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.lineplot(data=dfx, x='date', y='sales', hue='store_nbr', alpha=0.5)\n",
    "plt.title('PRODUCE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0347b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx_weekly = (\n",
    "    dfx\n",
    "    .assign(date=pd.to_datetime(dfx[\"date\"]))\n",
    "    .set_index(\"date\")\n",
    "    .groupby(\"family\")[\"sales\"]\n",
    "    .resample(\"W\")\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316fdcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    dfx_weekly,\n",
    "    col=\"family\",\n",
    "    col_wrap=5,\n",
    "    height=1.4,\n",
    "    aspect=3.0,     # wider figure\n",
    "    sharex=True,\n",
    "    sharey=False\n",
    ")\n",
    "\n",
    "g.map_dataframe(sns.lineplot, x=\"date\", y=\"sales\")\n",
    "\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Week\", \"Weekly Sales\")\n",
    "g.fig.subplots_adjust(top=0.95)\n",
    "g.fig.suptitle(\"Weekly Sales by Family\", fontsize=16)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49432c0f",
   "metadata": {},
   "source": [
    "- We have some families that have late-starts, for example this one with sales from 2014. Also some period of no sales in 2014.\n",
    "- Let's focus on the late-starts as the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Identify first sale date for each store-family combination\n",
    "store_family_start = df.filter(F.col(\"sales\") > 0).groupBy(\"store_nbr\", \"family\").agg(\n",
    "    F.min(\"date\").alias(\"active_start_date\"),\n",
    "    F.count(\"*\").alias(\"active_days\")\n",
    ")\n",
    "\n",
    "print(f\"Store-family combinations with sales: {store_family_start.count()}\")\n",
    "store_family_start.show(50)\n",
    "\n",
    "# Step 2: Join back to main df and filter\n",
    "df_with_start = df.join(store_family_start, on=[\"store_nbr\", \"family\"], how=\"inner\")\n",
    "\n",
    "# Only keep data from active_start_date onwards for each combo\n",
    "df_active = df_with_start.filter(F.col(\"date\") >= F.col(\"active_start_date\"))\n",
    "\n",
    "print(f\"\\nOriginal rows: {df.count():,}\")\n",
    "print(f\"Active period rows: {df_active.count():,}\")\n",
    "\n",
    "# Step 3: Filter out combinations with insufficient data\n",
    "# Require at least 365 days of data for training\n",
    "sufficient_data = df_active.groupBy(\"store_nbr\", \"family\").agg(\n",
    "    F.countDistinct(\"date\").alias(\"days_count\")\n",
    ").filter(F.col(\"days_count\") >= 365)\n",
    "\n",
    "print(f\"Combinations with >=365 days: {sufficient_data.count()}\")\n",
    "\n",
    "df_trainable = df_active.join(\n",
    "    sufficient_data.select(\"store_nbr\", \"family\"),\n",
    "    on=[\"store_nbr\", \"family\"],\n",
    "    how=\"inner\"\n",
    ").drop(\"active_start_date\", \"active_days\")\n",
    "\n",
    "print(f\"Final trainable rows: {df_trainable.count():,}\")\n",
    "\n",
    "# Now train models per family as before, but using df_trainable\n",
    "feature_cols = [\n",
    "    'onpromotion', 'is_holiday', 'day_of_week', 'month', 'year',\n",
    "    'sales_lag_7', 'sales_lag_14', 'sales_rolling_mean_7',\n",
    "    'promo_rolling_sum_7', 'week_of_year', 'is_weekend',\n",
    "    'strIndxer_city', 'strIndxer_state', 'strIndxer_type', 'cluster'\n",
    "    # Note: NOT including strIndxer_family since we're training per family\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in feature_cols if col in df_trainable.columns]\n",
    "\n",
    "# Get families\n",
    "families = [row['family'] for row in df_trainable.select(\"family\").distinct().collect()]\n",
    "print(f\"\\nTraining models for {len(families)} families\")\n",
    "\n",
    "all_metrics = []\n",
    "successful_models = 0\n",
    "\n",
    "for idx, family in enumerate(families, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{idx}/{len(families)}] Training: {family}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Filter for this family\n",
    "        df_family = df_trainable.filter(F.col(\"family\") == family)\n",
    "        \n",
    "        # For each store-family, we're already using only their active period\n",
    "        count = df_family.count()\n",
    "        print(f\"Total rows: {count:,}\")\n",
    "        \n",
    "        if count < 1000:\n",
    "            print(f\"⚠️  Skipping - insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # Clean data\n",
    "        df_clean = df_family.select(['date', 'sales', 'store_nbr'] + feature_cols).na.drop()\n",
    "        # df_clean = df_family.select(['date', 'sales'] + feature_cols).na.drop()\n",
    "        clean_count = df_clean.count()\n",
    "        print(f\"Clean rows: {clean_count:,}\")\n",
    "        \n",
    "        # Get date range for THIS family's data\n",
    "        date_range = df_clean.agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "        family_min_date = date_range[0]\n",
    "        family_max_date = date_range[1]\n",
    "        \n",
    "        # Calculate 80/20 split based on THIS family's date range\n",
    "        date_span = (family_max_date - family_min_date).days\n",
    "        split_date = family_min_date + pd.Timedelta(days=int(date_span * 0.8))\n",
    "        \n",
    "        print(f\"Date range: {family_min_date} to {family_max_date}\")\n",
    "        print(f\"Split date: {split_date}\")\n",
    "        \n",
    "        # Assemble features\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df_assembled = assembler.transform(df_clean)\n",
    "        \n",
    "        # Time-based split\n",
    "        train_df = df_assembled.filter(F.col(\"date\") < split_date).cache()\n",
    "        test_df = df_assembled.filter(F.col(\"date\") >= split_date).cache()\n",
    "        \n",
    "        train_count = train_df.count()\n",
    "        test_count = test_df.count()\n",
    "        print(f\"Train: {train_count:,} | Test: {test_count:,}\")\n",
    "        \n",
    "        if train_count < 100 or test_count < 20:\n",
    "            print(f\"⚠️  Skipping - insufficient split data\")\n",
    "            train_df.unpersist()\n",
    "            test_df.unpersist()\n",
    "            continue\n",
    "        \n",
    "        # Train model\n",
    "        rf = RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"sales\",\n",
    "            numTrees=50,\n",
    "            maxDepth=8,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        model = rf.fit(train_df)\n",
    "        \n",
    "        print(\"Making predictions...\")\n",
    "        predictions = model.transform(test_df)\n",
    "        \n",
    "        # Evaluate\n",
    "        evaluator_rmse = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        evaluator_mae = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "        evaluator_r2 = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "        \n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        mae = evaluator_mae.evaluate(predictions)\n",
    "        r2 = evaluator_r2.evaluate(predictions)\n",
    "        \n",
    "        print(f\"✓ Results: RMSE={rmse:.2f} | MAE={mae:.2f} | R²={r2:.4f}\")\n",
    "        \n",
    "        # Save metrics\n",
    "        all_metrics.append({\n",
    "            'family': family,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'train_rows': train_count,\n",
    "            'test_rows': test_count,\n",
    "            'date_range_days': date_span\n",
    "        })\n",
    "        \n",
    "        successful_models += 1\n",
    "        \n",
    "        # Clean up\n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully trained: {successful_models}/{len(families)} models\")\n",
    "\n",
    "if all_metrics:\n",
    "    metrics_df = pd.DataFrame(all_metrics).sort_values('r2')\n",
    "    print(\"\\n\" + metrics_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n📊 Overall Performance:\")\n",
    "    print(f\"   Average RMSE: {metrics_df['rmse'].mean():.2f}\")\n",
    "    print(f\"   Average MAE: {metrics_df['mae'].mean():.2f}\")\n",
    "    print(f\"   Average R²: {metrics_df['r2'].mean():.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df.to_csv('./model_metrics_active_periods.csv', index=False)\n",
    "    print(f\"\\n✓ Metrics saved to ./model_metrics_active_periods.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efa6fc",
   "metadata": {},
   "source": [
    "Our average R² increased from 0.4151 to 0.5013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eca10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_metrics = os.listdir('.')\n",
    "\n",
    "metrics_dfs = []\n",
    "for model in model_metrics:\n",
    "    if model.startswith('model_metrics'):\n",
    "        print(f\"Model metrics: {model}\")\n",
    "        metrics_df = pd.read_csv(model)\n",
    "        metrics_df['model'] = model.replace('.csv', '')\n",
    "        metrics_dfs.append(metrics_df)\n",
    "        # print(metrics_df.to_string(index=False))\n",
    "\n",
    "metrics_dfs = pd.concat(metrics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    metrics_dfs\n",
    "    .pivot_table(\n",
    "        index='family',\n",
    "        columns='model',\n",
    "        values='r2',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    .reset_index()\n",
    "    .round(2).sort_values('model_metrics_active_periods', ascending=False)\n",
    ")\n",
    "\n",
    "long = data.melt(\n",
    "    id_vars='family',\n",
    "    value_vars=[\n",
    "        'model_metrics_families_all_periods',\n",
    "        'model_metrics_active_periods'\n",
    "    ],\n",
    "    var_name='period',\n",
    "    value_name='r2'\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    data=long,\n",
    "    x='family',\n",
    "    y='r2',\n",
    "    hue='period',\n",
    "    palette=['lightblue', 'lightgreen']\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('R²')\n",
    "plt.xlabel('')\n",
    "plt.legend(title='');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f902d6e",
   "metadata": {},
   "source": [
    "- There are some improvements by removing the late-start periods.\n",
    "- However, we also noticed there are some issues with large date gaps. Let's try to fix those next.\n",
    "- And let's focus on HOME AND KITCHEN I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df.filter(F.col('family') == 'HOME AND KITCHEN I').groupBy('date', 'store_nbr', 'family').agg(F.sum('sales').alias('sales')).toPandas()\n",
    "\n",
    "dfx_weekly = (\n",
    "    dfx\n",
    "    .assign(date=pd.to_datetime(dfx[\"date\"]))\n",
    "    .groupby([\"family\", \"store_nbr\"])  # include store\n",
    "    .resample(\"W\", on=\"date\")      # resample weekly\n",
    "    .sales\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90931fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "family_name = \"HOME AND KITCHEN I\"\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    dfx_weekly[dfx_weekly[\"family\"] == family_name],\n",
    "    col=\"store_nbr\",\n",
    "    col_wrap=5,\n",
    "    height=1.4,\n",
    "    aspect=3.0,\n",
    "    sharex=True,\n",
    "    sharey=False\n",
    ")\n",
    "\n",
    "g.map_dataframe(sns.lineplot, x=\"date\", y=\"sales\")\n",
    "\n",
    "g.set_titles(\"Store {col_name}\")\n",
    "g.set_axis_labels(\"Week\", \"Weekly Sales\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5516b",
   "metadata": {},
   "source": [
    "There are some stores with late-start after 2015. However, our previous model started from 2014-01-01.\n",
    "\n",
    "\n",
    "- [19/32] Training: HOME AND KITCHEN I\n",
    "- Total rows: 67,076\n",
    "- Clean rows: 67,076\n",
    "- Date range: 2014-01-01 to 2017-08-15\n",
    "- Split date: 2016-11-23\n",
    "- Train: 53,031 | Test: 14,045\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.parquet('../notebooks/data/train.parquet')\n",
    "# df = df.filter(F.col('family') == 'HOME AND KITCHEN I')\n",
    "# df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb40eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Phase 1: Find \"stable start date\" for each store-family\n",
    "# Define stable as: at least 7 consecutive days with sales in a 10-day window\n",
    "\n",
    "window_10d = Window.partitionBy(\"store_nbr\", \"family\").orderBy(\"date\").rowsBetween(-9, 0)\n",
    "\n",
    "df_with_density = df.withColumn(\n",
    "    \"sales_in_last_10_days\",\n",
    "    F.sum(F.when(F.col(\"sales\") > 0, 1).otherwise(0)).over(window_10d)\n",
    ")\n",
    "\n",
    "# Mark where stable period begins (7+ sales days in last 10 days)\n",
    "df_stable_flag = df_with_density.withColumn(\n",
    "    \"is_stable\",\n",
    "    F.when(F.col(\"sales_in_last_10_days\") >= 7, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Get first stable date for each store-family\n",
    "stable_starts = df_stable_flag.filter(F.col(\"is_stable\") == 1) \\\n",
    "    .groupBy(\"store_nbr\", \"family\") \\\n",
    "    .agg(F.min(\"date\").alias(\"stable_start_date\"))\n",
    "\n",
    "print(\"Sample stable start dates:\")\n",
    "stable_starts.show(54)\n",
    "\n",
    "# Phase 2: Use only stable period data\n",
    "df_stable = df.join(stable_starts, on=[\"store_nbr\", \"family\"], how=\"inner\") \\\n",
    "              .filter(F.col(\"date\") >= F.col(\"stable_start_date\"))\n",
    "\n",
    "print(f\"\\nOriginal rows: {df.count():,}\")\n",
    "print(f\"Stable period rows: {df_stable.count():,}\")\n",
    "\n",
    "# Phase 3: Require minimum data for training\n",
    "sufficient_data = df_stable.groupBy(\"store_nbr\", \"family\").agg(\n",
    "    F.countDistinct(\"date\").alias(\"days_count\"),\n",
    "    F.sum(\"sales\").alias(\"total_sales\")\n",
    ").filter(\n",
    "    (F.col(\"days_count\") >= 365) &  # At least 1 year of STABLE data\n",
    "    (F.col(\"total_sales\") >= 0)\n",
    ")\n",
    "\n",
    "print(f\"Store-family combos with sufficient stable data: {sufficient_data.count()}\")\n",
    "\n",
    "df_trainable = df_stable.join(\n",
    "    sufficient_data.select(\"store_nbr\", \"family\"),\n",
    "    on=[\"store_nbr\", \"family\"],\n",
    "    how=\"inner\"\n",
    ").drop(\"stable_start_date\")\n",
    "\n",
    "print(f\"Final trainable rows: {df_trainable.count():,}\")\n",
    "df_trainable.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# feature_cols = [\n",
    "#     'onpromotion', 'is_holiday', 'day_of_week', 'day_of_month', 'month', 'year',\n",
    "#     'sales_lag_7', 'sales_lag_14', 'sales_lag_30',\n",
    "#     'sales_rolling_mean_7', 'sales_rolling_mean_14', 'sales_rolling_std_7',\n",
    "#     'promo_rolling_sum_7', 'week_of_year', 'quarter', 'is_weekend',\n",
    "#     'is_month_start', 'is_month_end', 'is_salary_day',\n",
    "#     'strIndxer_city', 'strIndxer_state', 'strIndxer_type', 'cluster'\n",
    "# ]\n",
    "\n",
    "feature_cols = [\n",
    " 'onpromotion',\n",
    " 'cluster',\n",
    " 'is_holiday',\n",
    " 'dcoilwtico',\n",
    "#  'hash_features',\n",
    " 'strIndxer_family',\n",
    " 'strIndxer_city',\n",
    " 'strIndxer_state',\n",
    " 'strIndxer_type',\n",
    " 'day_of_week',\n",
    " 'day_of_month',\n",
    " 'month',\n",
    " 'year',\n",
    " 'is_salary_day',\n",
    " 'sales_lag_7',\n",
    " 'sales_lag_14',\n",
    " 'sales_lag_30',\n",
    " 'sales_rolling_mean_7',\n",
    " 'sales_rolling_mean_14',\n",
    " 'sales_rolling_std_7',\n",
    " 'promo_rolling_sum_7',\n",
    " 'week_of_year',\n",
    " 'quarter',\n",
    " 'is_weekend',\n",
    " 'is_month_start',\n",
    " 'is_month_end']\n",
    "\n",
    "\n",
    "# feature_cols = [\n",
    "#     'onpromotion', 'is_holiday', 'day_of_week', 'month', 'year',\n",
    "#     'sales_lag_7', 'sales_lag_14', 'sales_rolling_mean_7',\n",
    "#     'promo_rolling_sum_7', 'week_of_year', 'is_weekend',\n",
    "#     'strIndxer_city', 'strIndxer_state', 'strIndxer_type', 'cluster'\n",
    "#     # Note: NOT including strIndxer_family since we're training per family\n",
    "# ]\n",
    "\n",
    "\n",
    "feature_cols = [col for col in feature_cols if col in df_trainable.columns]\n",
    "\n",
    "families = [row['family'] for row in df_trainable.select(\"family\").distinct().collect()]\n",
    "print(f\"\\nTraining models for {len(families)} families\")\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for idx, family in enumerate(families, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{idx}/{len(families)}] Training: {family}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        df_family = df_trainable.filter(F.col(\"family\") == family)\n",
    "        count = df_family.count()\n",
    "        print(f\"Total rows (stable periods only): {count:,}\")\n",
    "        \n",
    "        if count < 1000:\n",
    "            print(f\"⚠️  Skipping - insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # Each store-family is already filtered to its stable period\n",
    "        # Check date range\n",
    "        date_range = df_family.agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "        print(f\"Date range: {date_range[0]} to {date_range[1]}\")\n",
    "        \n",
    "        # Clean and prepare\n",
    "        df_clean = df_family.select(['date', 'sales', 'store_nbr'] + feature_cols).na.drop()\n",
    "        \n",
    "        # Calculate split date (80/20)\n",
    "        date_span = (date_range[1] - date_range[0]).days\n",
    "        split_date = date_range[0] + pd.Timedelta(days=int(date_span * 0.8))\n",
    "        print(f\"Split date: {split_date}\")\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df_assembled = assembler.transform(df_clean)\n",
    "        \n",
    "        train_df = df_assembled.filter(F.col(\"date\") < split_date).cache()\n",
    "        test_df = df_assembled.filter(F.col(\"date\") >= split_date).cache()\n",
    "        \n",
    "        train_count = train_df.count()\n",
    "        test_count = test_df.count()\n",
    "        print(f\"Train: {train_count:,} | Test: {test_count:,}\")\n",
    "        \n",
    "        if train_count < 100 or test_count < 20:\n",
    "            print(f\"⚠️  Skipping - insufficient split\")\n",
    "            train_df.unpersist()\n",
    "            test_df.unpersist()\n",
    "            continue\n",
    "        \n",
    "        # Train\n",
    "        rf = RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"sales\",\n",
    "            numTrees=50,\n",
    "            maxDepth=8,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        model = rf.fit(train_df)\n",
    "        predictions = model.transform(test_df)\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(predictions)\n",
    "        mae = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(predictions)\n",
    "        r2 = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "        \n",
    "        print(f\"✓ RMSE={rmse:.2f} | MAE={mae:.2f} | R²={r2:.4f}\")\n",
    "        \n",
    "        all_metrics.append({\n",
    "            'family': family,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'train_rows': train_count,\n",
    "            'test_rows': test_count\n",
    "        })\n",
    "        \n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "# Summary\n",
    "if all_metrics:\n",
    "    metrics_df = pd.DataFrame(all_metrics).sort_values('rmse')\n",
    "    print(\"\\n\" + metrics_df.to_string(index=False))\n",
    "    print(f\"\\n📊 Average RMSE: {metrics_df['rmse'].mean():.2f}\")\n",
    "    print(f\"📊 Average R²: {metrics_df['r2'].mean():.4f}\")\n",
    "    metrics_df.to_csv('./metrics_stable_periods.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "feature_cols = [\n",
    " 'onpromotion',\n",
    " 'cluster',\n",
    " 'is_holiday',\n",
    " 'dcoilwtico',\n",
    "#  'hash_features',\n",
    " 'strIndxer_family',\n",
    " 'strIndxer_city',\n",
    " 'strIndxer_state',\n",
    " 'strIndxer_type',\n",
    " 'day_of_week',\n",
    " 'day_of_month',\n",
    " 'month',\n",
    " 'year',\n",
    " 'is_salary_day',\n",
    " 'sales_lag_7',\n",
    " 'sales_lag_14',\n",
    " 'sales_lag_30',\n",
    " 'sales_rolling_mean_7',\n",
    " 'sales_rolling_mean_14',\n",
    " 'sales_rolling_std_7',\n",
    " 'promo_rolling_sum_7',\n",
    " 'week_of_year',\n",
    " 'quarter',\n",
    " 'is_weekend',\n",
    " 'is_month_start',\n",
    " 'is_month_end']\n",
    "\n",
    "\n",
    "# feature_cols = [\n",
    "#     'onpromotion', 'is_holiday', 'day_of_week', 'month', 'year',\n",
    "#     'sales_lag_7', 'sales_lag_14', 'sales_rolling_mean_7',\n",
    "#     'promo_rolling_sum_7', 'week_of_year', 'is_weekend',\n",
    "#     'strIndxer_city', 'strIndxer_state', 'strIndxer_type', 'cluster'\n",
    "# ]\n",
    "\n",
    "feature_cols = [col for col in feature_cols if col in df_trainable.columns]\n",
    "print(f\"Available features: {feature_cols}\")\n",
    "\n",
    "families = [row['family'] for row in df_trainable.select(\"family\").distinct().collect()]\n",
    "print(f\"\\nTraining models for {len(families)} families\")\n",
    "\n",
    "all_metrics = []\n",
    "skipped_families = []\n",
    "\n",
    "for idx, family in enumerate(families, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{idx}/{len(families)}] Training: {family}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        df_family = df_trainable.filter(F.col(\"family\") == family)\n",
    "        count = df_family.count()\n",
    "        print(f\"Total rows (stable periods only): {count:,}\")\n",
    "        \n",
    "        if count < 1000:\n",
    "            print(f\"⚠️  Skipping - insufficient data\")\n",
    "            skipped_families.append({'family': family, 'reason': 'insufficient_data', 'rows': count})\n",
    "            continue\n",
    "        \n",
    "        # Show sales statistics\n",
    "        sales_stats = df_family.agg(\n",
    "            F.min(\"sales\").alias(\"min_sales\"),\n",
    "            F.avg(\"sales\").alias(\"avg_sales\"),\n",
    "            F.max(\"sales\").alias(\"max_sales\"),\n",
    "            F.stddev(\"sales\").alias(\"std_sales\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"Sales stats: min={sales_stats['min_sales']:.1f}, avg={sales_stats['avg_sales']:.1f}, \"\n",
    "              f\"max={sales_stats['max_sales']:.1f}, std={sales_stats['std_sales']:.1f}\")\n",
    "        \n",
    "        # Check date range\n",
    "        date_range = df_family.agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "        print(f\"Date range: {date_range[0]} to {date_range[1]}\")\n",
    "        \n",
    "        # Clean and prepare\n",
    "        df_clean = df_family.select(['date', 'sales', 'store_nbr'] + feature_cols).na.drop()\n",
    "        clean_count = df_clean.count()\n",
    "        print(f\"After dropping nulls: {clean_count:,} rows ({100*clean_count/count:.1f}%)\")\n",
    "        \n",
    "        if clean_count < 1000:\n",
    "            print(f\"⚠️  Skipping - too many nulls\")\n",
    "            skipped_families.append({'family': family, 'reason': 'too_many_nulls', 'rows': clean_count})\n",
    "            continue\n",
    "        \n",
    "        # Calculate split date (80/20)\n",
    "        date_span = (date_range[1] - date_range[0]).days\n",
    "        split_date = date_range[0] + pd.Timedelta(days=int(date_span * 0.8))\n",
    "        print(f\"Split date: {split_date} ({date_span} days total)\")\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        df_assembled = assembler.transform(df_clean)\n",
    "        \n",
    "        train_df = df_assembled.filter(F.col(\"date\") < split_date).cache()\n",
    "        test_df = df_assembled.filter(F.col(\"date\") >= split_date).cache()\n",
    "        \n",
    "        train_count = train_df.count()\n",
    "        test_count = test_df.count()\n",
    "        print(f\"Train: {train_count:,} | Test: {test_count:,}\")\n",
    "        \n",
    "        if train_count < 100 or test_count < 20:\n",
    "            print(f\"⚠️  Skipping - insufficient split\")\n",
    "            skipped_families.append({'family': family, 'reason': 'insufficient_split', \n",
    "                                    'train': train_count, 'test': test_count})\n",
    "            train_df.unpersist()\n",
    "            test_df.unpersist()\n",
    "            continue\n",
    "        \n",
    "        # Train\n",
    "        rf = RandomForestRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"sales\",\n",
    "            numTrees=50,\n",
    "            maxDepth=8,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        model = rf.fit(train_df)\n",
    "        \n",
    "        print(\"Making predictions...\")\n",
    "        predictions = model.transform(test_df)\n",
    "        \n",
    "        # Show sample predictions\n",
    "        sample_preds = predictions.select(\"date\", \"sales\", \"prediction\").orderBy(\"date\").limit(10)\n",
    "        print(\"\\nSample predictions:\")\n",
    "        sample_preds.show(10, truncate=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(predictions)\n",
    "        mae = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(predictions)\n",
    "        r2 = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        pred_stats = predictions.agg(\n",
    "            F.avg(\"prediction\").alias(\"avg_pred\"),\n",
    "            F.stddev(\"prediction\").alias(\"std_pred\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\n✓ Results:\")\n",
    "        print(f\"  RMSE: {rmse:.2f}\")\n",
    "        print(f\"  MAE: {mae:.2f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  Avg actual sales: {sales_stats['avg_sales']:.1f}\")\n",
    "        print(f\"  Avg predicted sales: {pred_stats['avg_pred']:.1f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importances = model.featureImportances.toArray()\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': feature_importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 5 features:\")\n",
    "        for _, row in importance_df.head(5).iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        all_metrics.append({\n",
    "            'family': family,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'train_rows': train_count,\n",
    "            'test_rows': test_count,\n",
    "            'avg_sales': sales_stats['avg_sales'],\n",
    "            'avg_prediction': pred_stats['avg_pred']\n",
    "        })\n",
    "        \n",
    "        train_df.unpersist()\n",
    "        test_df.unpersist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        skipped_families.append({'family': family, 'reason': 'error', 'error': str(e)})\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_metrics:\n",
    "    metrics_df = pd.DataFrame(all_metrics).sort_values('r2', ascending=False)\n",
    "    print(\"\\n\" + metrics_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n📊 Performance Statistics:\")\n",
    "    print(f\"   Average RMSE: {metrics_df['rmse'].mean():.2f}\")\n",
    "    print(f\"   Average MAE: {metrics_df['mae'].mean():.2f}\")\n",
    "    print(f\"   Average R²: {metrics_df['r2'].mean():.4f}\")\n",
    "    print(f\"   Median R²: {metrics_df['r2'].median():.4f}\")\n",
    "    print(f\"   Best R²: {metrics_df['r2'].max():.4f} ({metrics_df.iloc[0]['family']})\")\n",
    "    print(f\"   Worst R²: {metrics_df['r2'].min():.4f} ({metrics_df.iloc[-1]['family']})\")\n",
    "    \n",
    "    # Save\n",
    "    metrics_df.to_csv('./metrics_stable_periods.csv', index=False)\n",
    "    print(f\"\\n✓ Metrics saved to ./metrics_stable_periods.csv\")\n",
    "else:\n",
    "    print(\"❌ No models successfully trained!\")\n",
    "\n",
    "if skipped_families:\n",
    "    print(f\"\\n⚠️  Skipped {len(skipped_families)} families:\")\n",
    "    skipped_df = pd.DataFrame(skipped_families)\n",
    "    print(skipped_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90edc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic 1: Check if test data overlaps with training somehow\n",
    "print(\"=== CHECKING TRAIN/TEST SPLIT ===\")\n",
    "\n",
    "df_family = df_trainable.filter(F.col(\"family\") == \"HOME AND KITCHEN I\")\n",
    "\n",
    "date_range = df_family.agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "print(f\"Full date range: {date_range[0]} to {date_range[1]}\")\n",
    "\n",
    "split_date = date_range[0] + pd.Timedelta(days=int((date_range[1] - date_range[0]).days * 0.8))\n",
    "print(f\"Split date: {split_date}\")\n",
    "\n",
    "train_dates = df_family.filter(F.col(\"date\") < split_date).agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "test_dates = df_family.filter(F.col(\"date\") >= split_date).agg(F.min(\"date\"), F.max(\"date\")).collect()[0]\n",
    "\n",
    "print(f\"Train: {train_dates[0]} to {train_dates[1]}\")\n",
    "print(f\"Test: {test_dates[0]} to {test_dates[1]}\")\n",
    "\n",
    "# Diagnostic 2: Check sales distribution in train vs test\n",
    "print(\"\\n=== SALES DISTRIBUTION ===\")\n",
    "train_stats = df_family.filter(F.col(\"date\") < split_date).agg(\n",
    "    F.avg(\"sales\").alias(\"avg\"),\n",
    "    F.stddev(\"sales\").alias(\"std\"),\n",
    "    F.min(\"sales\").alias(\"min\"),\n",
    "    F.max(\"sales\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "test_stats = df_family.filter(F.col(\"date\") >= split_date).agg(\n",
    "    F.avg(\"sales\").alias(\"avg\"),\n",
    "    F.stddev(\"sales\").alias(\"std\"),\n",
    "    F.min(\"sales\").alias(\"min\"),\n",
    "    F.max(\"sales\").alias(\"max\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Train sales: avg={train_stats['avg']:.1f}, std={train_stats['std']:.1f}, min={train_stats['min']:.1f}, max={train_stats['max']:.1f}\")\n",
    "print(f\"Test sales:  avg={test_stats['avg']:.1f}, std={test_stats['std']:.1f}, min={test_stats['min']:.1f}, max={test_stats['max']:.1f}\")\n",
    "\n",
    "# Diagnostic 3: Check for nulls in lag features\n",
    "print(\"\\n=== NULL CHECK ===\")\n",
    "df_clean = df_family.select(['date', 'sales'] + feature_cols).na.drop()\n",
    "print(f\"Before na.drop(): {df_family.count():,} rows\")\n",
    "print(f\"After na.drop(): {df_clean.count():,} rows\")\n",
    "print(f\"Dropped: {df_family.count() - df_clean.count():,} rows ({100*(df_family.count() - df_clean.count())/df_family.count():.1f}%)\")\n",
    "\n",
    "# Diagnostic 4: Look at actual predictions\n",
    "print(\"\\n=== SAMPLE PREDICTIONS ===\")\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_clean)\n",
    "\n",
    "train_df = df_assembled.filter(F.col(\"date\") < split_date)\n",
    "test_df = df_assembled.filter(F.col(\"date\") >= split_date)\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"sales\", numTrees=50, maxDepth=8, seed=42)\n",
    "model = rf.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(\"date\", \"sales\", \"prediction\", \"sales_lag_7\", \"sales_rolling_mean_7\") \\\n",
    "    .orderBy(\"date\").show(30, truncate=False)\n",
    "\n",
    "# Check for any extreme predictions\n",
    "print(\"\\n=== EXTREME PREDICTIONS ===\")\n",
    "predictions.select(\n",
    "    F.min(\"prediction\").alias(\"min_pred\"),\n",
    "    F.max(\"prediction\").alias(\"max_pred\"),\n",
    "    F.avg(\"prediction\").alias(\"avg_pred\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93a7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databricks_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
