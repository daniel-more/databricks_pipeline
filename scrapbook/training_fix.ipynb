{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e96b12c-c044-454c-9a05-db11410fef3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['databricks']['catalog'], config['databricks']['schema'], config[\"databricks\"][\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579ec654-a98f-4c6e-881c-5888b5135a29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def running_on_databricks():\n",
    "    \"\"\"Detect if running in Databricks environment\"\"\"\n",
    "    try:\n",
    "        import pyspark.dbutils  # only available in Databricks\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_DATABRICKS = running_on_databricks()\n",
    "print(IS_DATABRICKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e3be8f-13bb-4893-92ac-2f4f7b5c97b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# from helper import run_forecast, aggregate_to_granularity, build_features, train_test_split\n",
    "\n",
    "from helper import (\n",
    "    aggregate_to_granularity, assert_unique_series_rows, build_features,\n",
    "    train_test_split, model_factory, assemble_global_pipeline, fit_global_model, predict_global,\n",
    "    compute_metrics, fit_predict_local, rolling_backtest, run_forecast, plot_forecast, plot_train_test_forecast)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Give Spark way more memory since you have 32GB RAM available\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TimeSeriesForecast\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fce8f84-7812-42a1-9b24-cee0ba19bb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data (must include columns: date, sales, family, store_nbr)\n",
    "if IS_DATABRICKS:\n",
    "    df_raw = spark.read.format(\"delta\").table('portfolio_catalog.databricks_pipeline.silver_training').withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "\n",
    "else:\n",
    "    df_raw = (   \n",
    "        spark.read.parquet('../notebooks/data/train.parquet')\n",
    "\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"date\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310ef1ab-563b-4d23-8bbb-212dfcb4df32",
     "showTitle": false,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"family\":228},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766793718530}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"data\": {\"date_col\": \"date\", \"target_col\": \"sales\", \"group_cols\": [\"family\", \"store_nbr\"],\n",
    "             \"freq\": \"D\", \"min_train_periods\": 56},\n",
    "    \"aggregation\": {\"target_agg\": \"sum\", \"extra_numeric_aggs\": {\"dcoilwtico\": \"mean\", \"onpromotion\": \"sum\"}},\n",
    "    \"features\": {\"lags\": [1,7,14,28], \"mas\": [7,28], \"add_time_signals\": True},\n",
    "    \"split\": {\"mode\": \"horizon\", \"train_end_date\": \"\", \"test_horizon\": 28},\n",
    "    \"model\": {\"type\": \"spark_gbt\", \"params\": {\"maxDepth\": 7, \"maxIter\": 120}},\n",
    "    # \"model\": {\"type\": \"spark_lgbt\", \"params\": {\"maxDepth\": 7, \"maxIter\": 120}},\n",
    "    \"evaluation\": {\"mase_seasonality\": 7, \"backtest\": {\"enabled\": True, \"folds\": 4, \"fold_horizon\": 14, \"step\": 14}}\n",
    "}\n",
    "\n",
    "# --- Step 1: Features ---\n",
    "df_feat = build_features(df_raw, cfg[\"data\"][\"date_col\"], cfg[\"data\"][\"target_col\"],\n",
    "                         cfg[\"data\"][\"group_cols\"], cfg[\"features\"][\"lags\"], cfg[\"features\"][\"mas\"],\n",
    "                         cfg[\"features\"][\"add_time_signals\"], pre_aggregate=True,\n",
    "                         target_agg=cfg[\"aggregation\"][\"target_agg\"],\n",
    "                         extra_numeric_aggs=cfg[\"aggregation\"].get(\"extra_numeric_aggs\"))\n",
    "display(df_feat.limit(5))\n",
    "\n",
    "# --- Step 2: Split ---\n",
    "train, test = train_test_split(df_feat, cfg[\"data\"][\"date_col\"], cfg[\"data\"][\"group_cols\"],\n",
    "                               cfg[\"split\"][\"mode\"], cfg[\"split\"][\"train_end_date\"], cfg[\"split\"][\"test_horizon\"],\n",
    "                               cfg[\"data\"][\"min_train_periods\"])\n",
    "\n",
    "# --- Step 3: Train (global model) ---\n",
    "est = model_factory(cfg[\"model\"][\"type\"], cfg[\"model\"][\"params\"])\n",
    "feature_cols = [c for c in train.columns if c not in cfg[\"data\"][\"group_cols\"] + [cfg[\"data\"][\"date_col\"], cfg[\"data\"][\"target_col\"], \"label\"]]\n",
    "model = fit_global_model(train, cfg[\"data\"][\"target_col\"], cfg[\"data\"][\"group_cols\"], feature_cols, est)\n",
    "\n",
    "# --- Step 4: Predict ---\n",
    "pred = predict_global(model, test, cfg[\"data\"][\"group_cols\"], cfg[\"data\"][\"date_col\"], cfg[\"data\"][\"target_col\"])\n",
    "display(pred.limit(10))\n",
    "\n",
    "# --- Step 5: Metrics ---\n",
    "by_series, portfolio = compute_metrics(pred, cfg[\"data\"][\"date_col\"], \"y\", \"prediction\",\n",
    "                                       cfg[\"data\"][\"group_cols\"], cfg[\"evaluation\"][\"mase_seasonality\"])\n",
    "display(by_series.orderBy(\"wMAPE\")); display(portfolio)\n",
    "\n",
    "# # --- Optional: Backtest ---\n",
    "# from smartforecast.forecasting import aggregate_to_granularity, rolling_backtest\n",
    "# df_agg = aggregate_to_granularity(df_raw, cfg[\"data\"][\"date_col\"], cfg[\"data\"][\"target_col\"],\n",
    "#                                   cfg[\"data\"][\"group_cols\"], cfg[\"aggregation\"][\"target_agg\"],\n",
    "#                                   cfg[\"aggregation\"].get(\"extra_numeric_aggs\"))\n",
    "# bt = rolling_backtest(df_agg, cfg[\"data\"][\"date_col\"], cfg[\"data\"][\"target_col\"], cfg[\"data\"][\"group_cols\"],\n",
    "#                       feature_params={\"lags\": cfg[\"features\"][\"lags\"], \"mas\": cfg[\"features\"][\"mas\"], \"add_time_signals\": cfg[\"features\"][\"add_time_signals\"], \"freq\": cfg[\"data\"][\"freq\"]},\n",
    "#                       model_type=cfg[\"model\"][\"type\"], model_params=cfg[\"model\"][\"params\"],\n",
    "#                       folds=cfg[\"evaluation\"][\"backtest\"][\"folds\"], fold_horizon=cfg[\"evaluation\"][\"backtest\"][\"fold_horizon\"],\n",
    "#                       step=cfg[\"evaluation\"][\"backtest\"][\"step\"], mase_seasonality=cfg[\"evaluation\"][\"mase_seasonality\"])\n",
    "# display(bt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a504d6-fda9-40ba-88b5-71844fd56063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred.write.mode('overwrite').option(\"mergeSchema\", \"true\").saveAsTable(\n",
    "    'portfolio_catalog.databricks_pipeline.silver_validation_predictions'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "training_fix",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
